{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMQYA/YxEy4eAExl40NlZs4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielDialektico/dialektico-machine-learning-practices/blob/main/notebooks/Machine%20Learning/Aprendizaje%20Supervisado/descenso_de_gradiente.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://dialektico.com/wp-content/uploads/2023/03/MiniLogoW4.png\" alt=\"Dialéktico Logo\" />"
      ],
      "metadata": {
        "id": "ms6VNhA5VMKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Descenso de gradiente 🌠**\n",
        "---"
      ],
      "metadata": {
        "id": "fW2xRDMsVQQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introducción**"
      ],
      "metadata": {
        "id": "22eZjG8uVTi6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el recorrido [introductorio al descenso de gradiente](https://dialektico.com/introduccion-al-descenso-de-gradiente/) hemos revisado cómo es el funcionamiento interno de este artefacto para la optimización de **funciones de costo** y su uso para entrenamiento de modelos de **aprendizaje automático**.\n",
        "\n",
        "En esta práctica veremos cómo luce la evolución de los parámetros para un nuevo modelo, operando sobre un conjunto de datos que no habíamos utilizado anteriormente.  \n",
        "\n",
        "<br>\n",
        "<center><img src=\"https://dialektico.com/wp-content/uploads/2025/07/DGG_Colab.png\" width=\"300\" /></center>"
      ],
      "metadata": {
        "id": "TIfdetXDVYJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "LHAgeJWwXnHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Objetivo**"
      ],
      "metadata": {
        "id": "kMJBHShvXkV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El objetivo de esta práctica es entrenar un modelo de aprendizaje supervisado utilizando el algoritmo de descenso de gradiente.\n",
        "\n",
        "Debemos seguir los siguientes pasos:\n",
        "\n",
        "* Comprender el problema y realizar un análisis exploratorio de datos.\n",
        "* Aplicar preprocesamiento de datos.\n",
        "* Enrenar el modelo con descenso de gradiente.\n",
        "* Evaluar el modelo y generar un reporte de resultados.\n",
        "\n",
        "Con esto comprenderemos cómo es que el descenso de gradiente es un recurso útil para el entrenamiento de modelos que hemos explorado con anterioridad.\n",
        "\n",
        "<br>\n",
        "<center><img src=\"https://dialektico.com/wp-content/uploads/2025/08/DGG_Colab_mision.png\" width=\"400\" /></center>"
      ],
      "metadata": {
        "id": "bZV41gzkX3F5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "eXmuWJm2bGOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Planteamiento del problema"
      ],
      "metadata": {
        "id": "_313ftBASD7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta ocasión haremos uso de nuestras habilidades como científicxs de datos y atenderemos un nuevo caso: **predicción de tarifas de taxis**.\n",
        "\n",
        "Para esto utilizaremos un conjunto de datos con las siguientes características:\n",
        "* **Distancia en kilómetros** (Trip_Distance_km): La longitud del viaje en kilómeros.\n",
        "* **Momento del viaje** (Time_of_Day): Si se realizó durante la mañana, tarde o noche.\n",
        "* **Día de la semana** (Day_of_Week): Día de la semana en que se efectuó el viaje.\n",
        "* **Condiciones del tráfico** (Traffic_Conditions): Indicador de las caracerísticas del tráfico (ligero, medio, pesado).\n",
        "* **Cantidad de pasajeros** (Passenger_Count): Número de pasajeros para el viaje.\n",
        "* **Condición climática** (Weather): Datos categóricos del clima (despejado, lluvia, nieve).\n",
        "* **Duración del viaje en minutos** (Trip_Duration_Minutes): Tiempo total del viaje.\n",
        "* **Tarifa por kilómero en dólares** (Per_Km_Rate): La tarifa cobrada por kilómetro recorrido.\n",
        "* **Tarifa cobrada por minuto en dólares** (Per_Minute_Rate): La tarifa cobrada por minuto de duración del viaje.\n",
        "* **Tarifa base en dólares** (Base_Fare): La tarifa base inicial del trayecto en taxi antes de aplicar cualquier recargo por distancia o tiempo.\n",
        "* **Monto de la tarifa final en dólares** (Trip_Price): El costo del viaje.\n",
        "\n",
        "Nuestra misión es entrenar un modelo que, basado en las variables del conjunto de datos, nos permita estimar el costo de un viaje (monto de la tarifa)."
      ],
      "metadata": {
        "id": "VFWeaIk7SHPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<center><img src=\"https://dialektico.com/wp-content/uploads/2025/08/DDG_D2.jpg\" width=\"400\" /></center>"
      ],
      "metadata": {
        "id": "aD8QYFXITobQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Lo sabes? Dado que la tarifa es un número de tipo continuo, se trata de un problema de **regresión**, para lo cual podemos utilizar una regresión lineal multivariable."
      ],
      "metadata": {
        "id": "AL6vtGmETr-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "o7_4VEv3T3bf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparación de datos y librerías"
      ],
      "metadata": {
        "id": "y7tgxFD1bFyB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comenzaremos por cargar las librerías que utilizaremos. Volveremos a utilizar **mlektic**, la cual nos permite aplicar descenso de gradiente de manera sencilla, y obtener un reporte automatizado de resultados (la instalación puede tardar un poco)."
      ],
      "metadata": {
        "id": "NbHBQk0AUB13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Nota*:** *Las instalaciones se realizan a pesar de que algunas librerías ya están integradas de forma nativa en Colab, esto para asegurar que el Notebook no presente problemas de ejecución si se dan cambios en la sintaxis entre versiones de librerías.*"
      ],
      "metadata": {
        "id": "l7hoEI_1dw7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se instalan las librerías necesarias.\n",
        "!pip install mlektic==1.0.8"
      ],
      "metadata": {
        "id": "Sby_14AlVTza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se importan las librerías y se establecen configuraciones adicionales:"
      ],
      "metadata": {
        "id": "E61x-fZ7jwQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se importan las librerías.\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from mlektic.linear_reg import LinearRegressionArcht\n",
        "from mlektic.reporting import ReportBuilder\n",
        "from mlektic.plot_utils import plot_cost\n",
        "from mlektic import preprocessing, methods\n",
        "from google.colab import files\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "\n",
        "# Se filtran las advertencias.\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Se define el estilo de las gráficas.\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "# Se define el despliegue de flotantes en dataframes.\n",
        "pd.options.display.float_format = '{:.2f}'.format"
      ],
      "metadata": {
        "id": "9BmOjSybfTMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora cargamos el conjunto de datos con el que trabajaremos:"
      ],
      "metadata": {
        "id": "63T8VLZXgTHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se toma el conjunto de datos de un repositorio de GitHub.\n",
        "dataset = pd.read_csv('https://raw.githubusercontent.com/DanielDialektico/dialektico-machine-learning-practices/refs/heads/main/data/viajes_taxi.csv', encoding='latin1')\n",
        "dataset"
      ],
      "metadata": {
        "id": "vZekqrpYf60h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora mostramos información general de los datos:"
      ],
      "metadata": {
        "id": "JQl68RDTbNxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.info()"
      ],
      "metadata": {
        "id": "plzG9MbVVS1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se imprime una tabla con datos estadísticos de cada variable del conjuto de datos.\n",
        "dataset.describe()"
      ],
      "metadata": {
        "id": "-TZ1drypVRuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "HiqnxjSxicu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos notar información de valor de manera rápida:\n",
        "* Tenemos valores nulos en todas las variables, ya que deberían ser 1000 registros no nulos de cada una.\n",
        "* Existen valores no numéricos.\n",
        "* En algunas variables la desviación estándar es en apariencia alta.\n",
        "* Los mímimos y máximos globales están bastante separados (valores más altos y más bajos del conjunto)."
      ],
      "metadata": {
        "id": "UsembxvlbhPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "U90XFf3ib8mg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocesamiento de datos**"
      ],
      "metadata": {
        "id": "oPAfZfkejBh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Haremos un preprocesamiento rápido de los datos con las siguientes etapas:\n",
        "\n",
        "* Limpieza de valores nulos.\n",
        "* Transformación de datos categóricos en numéricos.\n",
        "* Limpieza de valores atípicos.\n",
        "* Estandarización de datos."
      ],
      "metadata": {
        "id": "BAdDbhQnb95o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Nota***: *es posible realizar diferentes análisis y etapas de preprocesamiento. Aquí lo haremos de forma sencilla para evitar extendernos en la práctica, pero eres libre de realizar las adecuaciones que consideres necesarias al conjunto de datos.*"
      ],
      "metadata": {
        "id": "V_11sCopcr3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "K7AlnNsk9MbP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limpieza de datos"
      ],
      "metadata": {
        "id": "kq6vxL0KdI9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comenzaremos por mostrar el número de valores nulos, a pesar de que pudimos vislumbrarlo en las tablas antes invocadas:"
      ],
      "metadata": {
        "id": "bg19O0m1diTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se imprimen los valores nulos de cada columna.\n",
        "dataset.isnull().sum()"
      ],
      "metadata": {
        "id": "k9aE2PaI4AAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En promedio tenemos 50 valores nulos por variable, lo cual representa un 5% de la muestra total. Dado que queremos utilizar un proceso de limpieza simple, eliminamos los valores nulos:"
      ],
      "metadata": {
        "id": "gjUNO0I_dyUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removemos los valores nulos.\n",
        "dataset_wnulls = dataset.dropna()\n",
        "\n",
        "# Mostramos los nulos.\n",
        "dataset_wnulls.isnull().sum()"
      ],
      "metadata": {
        "id": "BLvk6lW0dx0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora que hemos realizado la remoción de los datos nulos, buscaremos algún tipo de valor extraño en las variables categóricas (con valores cualitativos), mostrando sus valores únicos:"
      ],
      "metadata": {
        "id": "OLlHnybYeVIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se seleccionan columnas tipo object (valores cualitativos).\n",
        "obj_cols = dataset_wnulls.select_dtypes(include=\"object\").columns\n",
        "\n",
        "# Se imprimen los valores únicos de cada columna.\n",
        "for col in obj_cols:\n",
        "    print(f\"Columna: {col}\")\n",
        "    print(f\"Valores únicos: {dataset_wnulls[col].unique()}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "vEX0ZNbVeo9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hemos comprobado que los valores no contienen algún tipo de error de escritura, por lo que podemos trabajar ahora en sustituir estos por valores numéricos."
      ],
      "metadata": {
        "id": "cq-ckmX-e0uJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "KLhzfBb1fPmw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tansformación de datos"
      ],
      "metadata": {
        "id": "KI9euS_7fRNQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para transformar los datos utilizaremos la librería [scikit-learn](https://scikit-learn.org/stable/), con la función `OrdinalEncoder`, que asignará números a cada valor cualitativo basado en un orden que le hayamos dado a las variables. En este caso, es importante el orden, ya que los valores de las variables se relacionan de esta manera, por ejemplo: la mañana es antes de la noche, y la magnitud del tráfico va de menor a mayor."
      ],
      "metadata": {
        "id": "VVt0031gfsDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se seleccionan las columnas categóricas a codificar.\n",
        "ord_cols = [\"Time_of_Day\", \"Day_of_Week\", \"Traffic_Conditions\", \"Weather\"]\n",
        "\n",
        "# Se define el orde de las categorías.\n",
        "categories = [\n",
        "    [\"Morning\", \"Afternoon\", \"Evening\", \"Night\"],  # Tiempo del día.\n",
        "    [\"Weekday\", \"Weekend\"],                        # Día de la semana.\n",
        "    [\"Low\", \"Medium\", \"High\"],                     # Condiciones del tráfico.\n",
        "    [\"Clear\", \"Rain\", \"Snow\"],                     # Clima.\n",
        "]\n",
        "\n",
        "# Se instancia el codificador a utilizar.\n",
        "enc = OrdinalEncoder(categories=categories, encoded_missing_value=-1)\n",
        "\n",
        "# Se codifican los valores.\n",
        "dataset_enc = dataset_wnulls.copy()\n",
        "dataset_enc[ord_cols] = enc.fit_transform(dataset_enc[ord_cols])\n",
        "\n",
        "# Se muestra la tabla con los valores codificados.\n",
        "dataset_enc.head()"
      ],
      "metadata": {
        "id": "bK9e1eWyhn1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificamos que se haya asignado un número a cada valor categórico:"
      ],
      "metadata": {
        "id": "72FojHWhjhLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Columnas codificadas.\n",
        "cols = [\"Time_of_Day\", \"Day_of_Week\", \"Traffic_Conditions\", \"Weather\"]\n",
        "\n",
        "# Se imprimen los valores únicos.\n",
        "for col in cols:\n",
        "    print(f\"Columna: {col}\")\n",
        "    print(dataset_enc[col].unique())\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "8ymQsSRhi8mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "ififrNxXjtoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Revisión de correlación entre variables."
      ],
      "metadata": {
        "id": "ymSrG_d7jvSQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí añadiremos una rápido vistazo de la correlación entre las variables, esto nos permitirá ver si existen variables con una fuerte correlación, de tal forma que podamos eliminar algunas para evitar redundancia:"
      ],
      "metadata": {
        "id": "Zuz703rKku0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se obtiene la matriz de correlación.\n",
        "corr = dataset_enc.corr(method=\"pearson\")\n",
        "corr"
      ],
      "metadata": {
        "id": "hYIwqDovju9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observamos que existe poca correlación entre las variables, por lo que mantenemos todas para ser modeladas por la regresión lineal más adelante. En este sentido de limpieza de variables, exsten métodos más avanzados como el análisis de componentes principales (ACP), pero lo dejaremos para trabajos futuros (de cualquier manera, recuerda que eres libre de aplicar el preprocesamiento que gustes a tus datos)."
      ],
      "metadata": {
        "id": "1rIAlu7DlCeI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "7-hyrfeslSqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eliminación de datos atípicos"
      ],
      "metadata": {
        "id": "UzwDRFNolTi4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proseguimos con la eliminación de los valores atípicos. Idealmente, habría que revisar las distribuciones de los datos para decidir qué algoritmo utilizar para su detección, pero lo haremos directamente con un método robusto llamado Median Absolute Deviation (desviación absoluta de la mediana).\n",
        "\n",
        "\n",
        "Creamos nuestra función para detección de valores atípicos con numpy:"
      ],
      "metadata": {
        "id": "OWF3xuTSlW4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_outliers_mad(df, threshold=3.5):\n",
        "    \"\"\"\n",
        "    Elimina filas con outliers usando Median Absolute Deviation (MAD).\n",
        "    - threshold: valores mayores son más tolerantes (3.5 ≈ criterio habitual).\n",
        "    \"\"\"\n",
        "    df_clean = df.copy()\n",
        "    num_cols = df_clean.select_dtypes(include=\"number\").columns\n",
        "    mask = pd.Series(True, index=df_clean.index)\n",
        "\n",
        "    for col in num_cols:\n",
        "        x = df_clean[col]\n",
        "        med = x.median()\n",
        "        mad = (x - med).abs().median()\n",
        "        if mad == 0:\n",
        "            continue  # evita división por cero en columnas constantes\n",
        "        robust_z = 0.6745 * (x - med).abs() / mad\n",
        "        mask &= (robust_z <= threshold) | x.isna()\n",
        "\n",
        "    return df_clean[mask]\n"
      ],
      "metadata": {
        "id": "4WJg_Dcllxvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicamos la función a nuestro conjunto de datos:"
      ],
      "metadata": {
        "id": "wKc1L_vomFfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se aplica el método MAD al conjunto de datos.\n",
        "no_outliers_dataset = drop_outliers_mad(dataset_enc, threshold=3.5)\n",
        "\n",
        "# Se imprimen las dimensiones del conjunto de datos antes y después.\n",
        "print(\"Tamaño antes de la limpieza:\", dataset_enc.shape)\n",
        "print(\"Tamaño después de la limpieza:\", no_outliers_dataset.shape)"
      ],
      "metadata": {
        "id": "4hef0jJkl3JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El resultado ha sido la remoción de 14 valores atípicos, lo cual representa un muy bajo porcentaje del conjunto de datos (aprox. 2.5%)."
      ],
      "metadata": {
        "id": "hcA_TiiZmSxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "FwUPoiLzmymQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estandarización de datos"
      ],
      "metadata": {
        "id": "VWZ9ZO8amHl4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un paso típico en el preprocesamiento de los datos es la estandarización, la cual consiste en una transformación que convierte los valores de cada variable a una escala comparable, haciendo que:\n",
        "\n",
        "\\begin{align}\n",
        "        \\text{media}=0, \\text{desviación estándar=1} \\tag{1.1}\n",
        "    \\end{align}\n",
        "\n",
        "Cuando se utilizan métodos de optimización como descenso de gradiente, se recomienda estandarizar los datos, ya que tener las variables en una misma escala acelera la convergencia y evita pasos desbalanceados."
      ],
      "metadata": {
        "id": "ztPsTUcdo0qo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Procedemos a escalar los datos utilizando `StandardScaler()` de scikit-learn, donde cuidamos el no estandarizar la variable objetivo, ya que buscamos mantener las predicciones en la escala original:"
      ],
      "metadata": {
        "id": "VxMeaGVJqDkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se define la variable objetivo.\n",
        "target_col = \"Trip_Price\"\n",
        "\n",
        "# Se separan las variables de entrada y salida.\n",
        "X = no_outliers_dataset.drop(columns=[target_col])\n",
        "y = no_outliers_dataset[target_col]\n",
        "\n",
        "# Se escalan los datos.\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Se reconstruye la tabla de datos.\n",
        "preprocessed_dataset = pd.DataFrame(\n",
        "    X_scaled,\n",
        "    columns=X.columns,\n",
        "    index=no_outliers_dataset.index\n",
        ")\n",
        "\n",
        "preprocessed_dataset[target_col] = y\n",
        "\n",
        "# Se muestra la tabla.\n",
        "preprocessed_dataset\n"
      ],
      "metadata": {
        "id": "QDKqMDcundPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hemos concluido con las etapas de preprocesamiento, recuerda que aquí lo hemos dejado simple, pero un buen preprocesamiento implica análisis rigurosos, visualizaciones gráficas, y diversos procesos contextualizados a las características de los datos.\n",
        "\n"
      ],
      "metadata": {
        "id": "ugzXlWQeq5Io"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<center><img src=\"https://dialektico.com/wp-content/uploads/2025/08/DDG_K3.jpg\" width=\"400\" /></center>"
      ],
      "metadata": {
        "id": "mdS4aHJhrclQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "mBuHFgPIrnGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento del modelo"
      ],
      "metadata": {
        "id": "Q7J1WZ7bro84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para entrenar el modelo, utilizaremos una **regresión lineal** con mlektik, la cual utiliza [Pytorch](https://pytorch.org/) de forma subyacente para entrenar el modelo con descenso de gradiente. El descenso de gradiente se puede realizar con otras librerías, pero aquí lo haremos con mlektik por su simplicidad y transparencia en la aplicación a regresiones lineales y logísticas."
      ],
      "metadata": {
        "id": "PI53NRy5rrap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenaremos al modelo eligiendo los siguientes hiperparámetros explicados en el recorrido:\n",
        "* Tasa de aprendizaje = 0.05\n",
        "* Iteraciones = 10"
      ],
      "metadata": {
        "id": "xF0_UEppmv5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Además, utilizaremos el 80% de los datos para entrenamiento, y el 20% restante para evaluación, esto lo haremos con `preprocessing.pd_dataset` de mlektic."
      ],
      "metadata": {
        "id": "REaj45jNB6i2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separamos las variables de entrada de la variable objetivo.\n",
        "cols = preprocessed_dataset.columns.drop(\"Trip_Price\").tolist()\n",
        "\n",
        "# Se separa el conjunto de datos para entrenamiento y prueba.\n",
        "train_set, test_set = preprocessing.pd_dataset(\n",
        "    preprocessed_dataset,\n",
        "    input_columns = cols,\n",
        "    output_column = \"Trip_Price\",\n",
        "    train_fraction = 0.8,\n",
        ")\n",
        "\n",
        "# Se selecciona el optimizador, en este caso, un descenso de gradiente,\n",
        "# con la tasa de aprendizaje seleccionada.\n",
        "optimizer = methods.optimizer_archt(\"sgd-standard\", learning_rate=0.05)\n",
        "\n",
        "# Se prepara la fase de entrenamiento, configurando 10 iteraciones.\n",
        "linreg = LinearRegressionArcht(\n",
        "    method     = \"batch\",\n",
        "    optimizer  = optimizer,\n",
        "    iterations = 10,\n",
        "    metric = \"r2\"\n",
        ")\n",
        "\n",
        "# Se entrena el modelo.\n",
        "linreg.train(train_set);"
      ],
      "metadata": {
        "id": "pzOBxxcD3Pli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verás que se desglosaron algunos datos sobre la ejecución, estas son las definiciones:\n",
        "*   **Epoch**: época o número de iteración del entrenamiento.\n",
        "*   **Loss**: valor de la función de pérdida.\n",
        "*   **R2**: métrica $R^2$ para calcular el rendimiento del modelo.\n",
        "\n"
      ],
      "metadata": {
        "id": "8mRpBLSSni6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluamos el $R^2$ en el conjunto de pruebas:"
      ],
      "metadata": {
        "id": "jQgOE0q6pQL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linreg.eval(test_set, 'r2')"
      ],
      "metadata": {
        "id": "3_po2yRyATcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos notar que el $R^2$ indica un modelo deficiente, ya que, como hemos visto anteriormente, debería acercarse a $1$ para indicar un buen desempeño del modelo; por ello, buscaremos modificar los hiperparámetros utilizados."
      ],
      "metadata": {
        "id": "CKXeBHU9pbDw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "gZuvOwOnqMFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuración de hiperparámetros y generación de reportes"
      ],
      "metadata": {
        "id": "y9nWUwhpqN8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de hacer modificaciones en los parámetros, quiero que aprendas cómo generar un reporte automático del entrenamiento que acabas de ejecutar. Es bastante sencillo y rápido, simplementa se utiliza la función ```ReportBuilder``` de mlektic.\n",
        "\n",
        "Solo debes pasar el modelo que entrenaste (en este caso, `linreg`), y de manera opcional, la descripción de tu conjunto de datos y las etapas de preprocesamiento.\n",
        "\n",
        "Lo hacemos de la siguiente manera:\n",
        "\n"
      ],
      "metadata": {
        "id": "y74bD0ZsqWB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descripción opcional del conjunto de datos.\n",
        "dataset_description = \"Conjunto de datos de precios de viajes en Taxi\"\n",
        "\n",
        "# Descripción opcional del preprocesamiento de datos.\n",
        "preprocessing_steps = [\n",
        "    \"Limpieza de valores nulos.\",\n",
        "    \"Transformación de datos categóricos en numéricos.\",\n",
        "    \"Limpieza de valores atípicos.\",\n",
        "    \"Estandarización de datos.\",\n",
        "]\n",
        "\n",
        "# Se utiliza el constructor de reportes, pasando el modelo y descripciones.\n",
        "builder = ReportBuilder(\n",
        "    mdl          = linreg,\n",
        "    dataset_name = dataset_description,\n",
        "    preprocessing = preprocessing_steps,\n",
        ")\n",
        "\n",
        "# Se genera el reporte.\n",
        "builder.to_html(\"reporte.html\")\n",
        "\n",
        "# Se descarga.\n",
        "files.download(\"reporte.html\")\n",
        "print('Reporte generado y descargado.')"
      ],
      "metadata": {
        "id": "t4SCv818qf33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El reporte debería haberse descargado automáticamente, busca en tus descargas y abre el archivo *reporte.html*, donde encontrarás algo como esto:"
      ],
      "metadata": {
        "id": "odsWjNpxrz-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<center><img src=\"https://dialektico.com/wp-content/uploads/2025/08/DDG_report.png\" width=\"800\" /></center>"
      ],
      "metadata": {
        "id": "eXpQH30SsA43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<center><img src=\"https://dialektico.com/wp-content/uploads/2025/08/DDG_D3.png\" width=\"400\" /></center>"
      ],
      "metadata": {
        "id": "Zd2a8mVosTm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Desglosaremos con más detalle el reporte en lo subsiguiente, por el momento, buscaremos mejorar el desempeño del modelo.\n",
        "\n",
        "Haremos de nuevo el entrenamiento, pero modificando un hiperparámetro, el número de iteraciones:\n",
        "* Tasa de aprendizaje = 0.05\n",
        "* Iteraciones = 50 (nuevo valor)"
      ],
      "metadata": {
        "id": "moEiJqdpte2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = methods.optimizer_archt(\"sgd-standard\", learning_rate=0.05)\n",
        "\n",
        "# Se entrena el modelo, seleccionando\n",
        "linreg = LinearRegressionArcht(\n",
        "    method     = \"batch\",\n",
        "    optimizer  = optimizer,\n",
        "    iterations = 50, # Nuevo valor\n",
        "    metric     = 'r2'\n",
        ")\n",
        "\n",
        "linreg.train(train_set);"
      ],
      "metadata": {
        "id": "HBiblKfkJwID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostramos el valor de $R^2$ en el conjunto de datos de pruebas:"
      ],
      "metadata": {
        "id": "R_x1Q_VtucUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linreg.eval(test_set, 'r2')"
      ],
      "metadata": {
        "id": "SIa0wxbVuk1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hemos notado que el desempeño ha mejorado bastante, ya que $R^2$ se acerca mucho más a $1$ de lo que se acercaba anteriormente. Esto implica que el modelo ha mejorado al añadir un mayor número de iteraciones.\n",
        "\n",
        "Podemos observar cómo evolucionó el valor de la función de pérdida con la siguiente gráfica:"
      ],
      "metadata": {
        "id": "LrElYh1FulxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "cost_history = linreg.get_cost_history()\n",
        "plot_cost(cost_history, dim = (7, 5))"
      ],
      "metadata": {
        "id": "5TCwW2Fwu_T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puedes notar que a lo largo de las iteraciones el valor de la función de costo ha disminuido, por lo que el descenso de gradientes ha realizado bien su trabajo minimizando este valor a lo largo de las épocas."
      ],
      "metadata": {
        "id": "1D2zfEj2wq9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como último paso, genera tu reporte:"
      ],
      "metadata": {
        "id": "5MSnszckw0UY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se utiliza el constructor de reportes, pasando el modelo y descripciones.\n",
        "builder = ReportBuilder(\n",
        "    mdl          = linreg,\n",
        "    dataset_name = dataset_description,\n",
        "    preprocessing = preprocessing_steps,\n",
        ")\n",
        "\n",
        "# Se genera el reporte.\n",
        "builder.to_html(\"reporte.html\")\n",
        "\n",
        "# Se descarga\n",
        "files.download(\"reporte.html\")\n",
        "print('Reporte generado y descargado.')"
      ],
      "metadata": {
        "id": "-_wEBAcGw7O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "WLBLJr0nEZcO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Taxonomía del reporte"
      ],
      "metadata": {
        "id": "ioNDvgshEaV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abre el reporte y revisa cada una de las secciones:\n",
        "\n",
        "* **Detalles de ejecución**: detalles sobre el algoritmo utilizado.\n",
        "* **Conjunto de Datos**: información sobre el conjunto de datos y las variables utilizadas.\n",
        "* **Hiperparámetros y configuraciones**: hiperparámetros seleccionados para la ejecución del entrenamiento.\n",
        "* **Resultados**: métricas de evaluación para cada subconjunto de datos.\n",
        "* **Evolución de entrenamiento**: gráficas que muestran la evolución de los valores de la función de pérdida y la métrica de evaluación a lo largo de las iteraciones.\n"
      ],
      "metadata": {
        "id": "4jOrvliYxBEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y finalmente, dos secciones importantes para volver transparente el producto final del entrenamiento con machine learning:\n",
        "\n",
        "* **Muestreo de parámetros durante entrenamiento**: aquí puedes ver cómo cambiaron los valores de algunos parámetros de tu modelo. Se inició con valores aleadorios y fueron modificados por el descenso de gradiente.\n"
      ],
      "metadata": {
        "id": "oI4fH29jx2yA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<center><img src=\"https://dialektico.com/wp-content/uploads/2025/08/DDG_report_2.png\" width=\"800\" /></center>"
      ],
      "metadata": {
        "id": "ridqHgngyPuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y la última sección:\n",
        "* **Modelo obtenido**: muestra la forma final del modelo que has entrenado y utilizas para hacer predicciones con datos nuevos.\n"
      ],
      "metadata": {
        "id": "gv5eiVFeytEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<center><img src=\"https://dialektico.com/wp-content/uploads/2025/08/DDG_report_3.png\" width=\"800\" /></center>"
      ],
      "metadata": {
        "id": "MXBcsjmiy-jo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con estos reportes puedes observar los detalles matemáticos de tus modelos de aprendizaje automático, manteniéndote conectado con las técnicas subyacentes que generan a la inteligencia artificial, y permitiendo compartir fácilmente resúmenes de tu trabajo."
      ],
      "metadata": {
        "id": "dVTBjxoIzC7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "MDGXl3FS0BZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejecicio (opcional)"
      ],
      "metadata": {
        "id": "wu6h_mw-0Fd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El ejercicio adicional es bastante sencillo. Modifica los hiperparámetros de entrenamiento para ver sus efectos en el rendimiento del modelo.\n",
        "\n",
        "El siguiente bloque entrena el modelo y genera el reporte de manera automática:"
      ],
      "metadata": {
        "id": "lWJT7aQM0R2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = methods.optimizer_archt(\"sgd-standard\",\n",
        "                                    learning_rate=0.05) # Tasa de aprendizaje.\n",
        "\n",
        "# Se entrena el modelo, seleccionando\n",
        "linreg = LinearRegressionArcht(\n",
        "    method     = \"batch\",\n",
        "    optimizer  = optimizer,\n",
        "    iterations = 50, # Número de iteraciones.\n",
        "    metric     = 'r2'\n",
        ")\n",
        "\n",
        "linreg.train(train_set);\n",
        "\n",
        "# Se utiliza el constructor de reportes, pasando el modelo y descripciones.\n",
        "builder = ReportBuilder(\n",
        "    mdl          = linreg,\n",
        "    dataset_name = dataset_description,\n",
        "    preprocessing = preprocessing_steps,\n",
        ")\n",
        "\n",
        "# Se genera el reporte.\n",
        "builder.to_html(\"reporte.html\")\n",
        "\n",
        "# Se descarga\n",
        "files.download(\"reporte.html\")\n",
        "print('\\nReporte generado y descargado.')"
      ],
      "metadata": {
        "id": "2ztBwmAM0KGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "c9LJTdAq0n4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La práctica ha concluido aquí. Has aplicado descenso de gradiente para ajustar los parámetros de una ecuación lineal, que permite estimar el precio de un viaje dado un conjunto de valores de variables."
      ],
      "metadata": {
        "id": "dPv9pJv8bTzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "▶ [Regresar a la lección](https://dialektico.com/descenso-de-gradiente/) 🧙"
      ],
      "metadata": {
        "id": "ixgkHEdvbQVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Apéndice"
      ],
      "metadata": {
        "id": "RFDjs55QEyym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conjunto de datos"
      ],
      "metadata": {
        "id": "uQs1NNZJE7Zd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El conjunto de datos utilizado se obtuvo de Kaggle, sus detalles pueden ser consultados en la siguiente liga: https://www.kaggle.com/datasets/denkuznetz/taxi-price-prediction/data"
      ],
      "metadata": {
        "id": "Qx4YKGa2E9al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dialektico Machine learning practices © 2024 by Daniel Antonio García Escobar\n",
        "# is licensed under CC BY-NC 4.0. To view a copy of this license,\n",
        "# visit https://creativecommons.org/licenses/by-nc/4.0/\n",
        "\n",
        "# Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International\n",
        "# Public License"
      ],
      "metadata": {
        "id": "Z7vBuq0sbdDp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}