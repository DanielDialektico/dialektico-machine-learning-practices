{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNN382JjnCuiVgL78H3j0ZU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielDialektico/dialektico-machine-learning-practices/blob/main/notebooks/Preprocesamiento/Preprocesamiento_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://dialektico.com/wp-content/uploads/2023/03/MiniLogoW4.png\" alt=\"Dialéktico Logo\" />"
      ],
      "metadata": {
        "id": "M_JD72l23F97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ANÁLISIS EXPLORATORIO Y PREPROCESAMIENTO DE DATOS CON PYTHON** ⚒\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tV0QYCqa0vT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introducción**\n",
        "El fin de esta práctica es darte un recorrido en el que observarás cómo se **analiza** y **preprocesa** un conjunto de datos con algunas librerías de **Python** (principalmente **Pandas**, la cual nos permite trabajar de forma sencilla con información dispuesta en tablas), con la pretensión de que experimentes las distintas etapas de edición por las que puede pasar un conjunto de datos antes de llegar a alimentar un modelo de **machine learning**.\n",
        "\n",
        "Ejecutarás código que te ayude a **analizar**, **limpiar** y **transformar** un conjunto de datos para generar uno más comprensible y digerible para la máquina, y/o para quien lo manipule y utilicé con algún fin analítico.\n",
        "\n",
        "  <center><img src=\"https://dialektico.com/wp-content/uploads/2024/05/Prepro_Eq.jpg\" width=\"300\" /></center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ShK24xsfCnBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "KWYvkDJ2SG4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Objetivo**\n",
        "Para esta práctica, se plantea el siguiente requerimiento:\n",
        "\n",
        "*  Se nos ha dado la tarea de **analizar** un conjunto de datos de automóviles para hallar **insights** (descubrimientos relevantes), y posteriormente crear un modelo de **machine learning** que será utilizado por el área de marketing. Sin embargo, sabemos que antes debemos de ejecutar una etapa de **preprocesamiento** para llevar a cabo estas tareas de manera eficiente, lo cual, a su vez, implica un análisis exploratorio de datos.\n",
        "*  Nuestro objetivo será la ejecución de esta etapa previa, realizando los cambios necesarios conforme a la información provista por el análisis.\n",
        "*  El conjunto de datos a trabajar es una tabla con **256 ejemplos de entrenamiento**, los cuales contienen **26 características** de cada coche.\n",
        "*  Para completar nuestra tarea, el equipo de marketing nos ha pedido que solo utilicemos los siguientes **atributos** (columnas): tipo de motor, número de cilindros, sistema de combustible, caballos de fuerza, y precio.\n",
        "\n",
        "<br>\n",
        "<center><img src=\"https://dialektico.com/wp-content/uploads/2024/10/PDD_Colab_png_2.png\" alt=\"Dialéktico Logo\" width=\"400\" /></center>"
      ],
      "metadata": {
        "id": "KqMzw_24Ey-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nota**: el conjunto de datos que utilizaremos comprende **información real** de acceso público. Para más información, consúltese el **[apéndice](https://colab.research.google.com/drive/10O1A-C6nKCfaCrb1Vu8ayfDuBtIIzE_9#scrollTo=HaVTloDvHZTr)** al final de la lección."
      ],
      "metadata": {
        "id": "KrUL9MMjv6m4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "cbhY8ixRSNws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Declaración de librerías y carga de datos**\n",
        "En este caso, el conjunto de datos se descargará desde un repositorio en **[GitHub](https://github.com/DanielDialektico)**. Se trata de una sola tabla, por lo que no será necesario acudir a una etapa de **integración** del **preprocesamiento** de datos.\n",
        "\n",
        "Para cargar el conjunto de datos a nuestro espacio de trabajo, se extraerá la información (alojada en formato CSV) utilizando la URL del archivo, y se almacenará en una **tabla** (dataFrame) utilizando la librería **Pandas**.\n",
        "\n",
        "Antes que nada, se importarán las **librerías** a utilizar, lo cual encontrarás al principio de la siguiente celda de código."
      ],
      "metadata": {
        "id": "v_xCEZn-QSwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://dialektico.com/wp-content/uploads/2024/10/PPD_M6.jpg\" alt=\"Dialéktico Logo\" width=\"600\" /></center>\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "Wq_2Fz9eSXRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecuta la siguiente celda para cargar los datos y las librerías, y de ahora en adelante, haz lo mismo con todos los bloques conforme vayan apareciendo:"
      ],
      "metadata": {
        "id": "Zf7_0kfdLeHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se importan las librerías.\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn import preprocessing\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "# Se define el tipo de formato de las gráficas a utilizar (se utilizarán más adelante).\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "# Se ignoran las alertas.\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Se carga el conjunto de datos desde GitHub\n",
        "dataset = pd.read_csv('https://raw.githubusercontent.com/DanielDialektico/dialektico-machine-learning-practices/refs/heads/main/data/Autom%C3%B3viles.csv')"
      ],
      "metadata": {
        "id": "ILh0ohbvSlQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprobamos que la carga se haya realizado correctamente mostrando los primeros cinco renglones de la tabla:"
      ],
      "metadata": {
        "id": "4BraKNkeT9iC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se especifica que se muestren todas las columnas de la tabla.\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Se imprimen los primeros cinco renglones.\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "CWbE5EpdUG1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "kdU2Fq7nSWkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Análisis inicial y limpieza de datos**\n",
        "Ahora que hemos cargado nuestro conjunto de datos, realizaremos una exploración inicial de los datos, para lo cual visualizaremos las características generales del conjunto.\n",
        "\n",
        "Primero, utilizaremos la funcion **[info()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html)** de Pandas, que imprimirá la siguiente información:  \n",
        "\n",
        "* **RangeIndex**: Número de renglones.\n",
        "* **Data Columns**: Número de columnas y características de columnas:\n",
        "    * **Column**: el nombre de cada columna.\n",
        "    * **Non-Null Count**: el número de valores no nulos, los cuales son los datos válidos, es decir, datos no vacíos o que no generan algún conflicto de lectura.\n",
        "    * **Dtype**: el tipo de datos en el que está categorizada cada columna.\n"
      ],
      "metadata": {
        "id": "4dbnsCpLPcZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Ejecuta la siguiente celda para visualizar esta información:"
      ],
      "metadata": {
        "id": "KrGiLGq0brZm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZLfS1_ihwKQ"
      },
      "outputs": [],
      "source": [
        "# Se despliega la información general del conjunto de datos.\n",
        "dataset.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta lista nos podemos percatar de que tenemos **26 columnas y 205 renglones**, y de que en algunas columnas tenemos la presencia de **valores nulos**, ya que el conteo \"non-null\" no siempre es igual a 205.\n",
        "\n",
        "El ejemplo más claro de esto es el de la columna \"**normalized-losses**\", la cual cuenta solo con 164 valores no nulos, indicando un total de 41 valores de este tipo presentes.\n"
      ],
      "metadata": {
        "id": "ozEqAzr7hAMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con esta información, y las instrucciones dadas en el planteamiento del **objetivo**, podemos comenzar la primera etapa del **[preprocesamiento](https://dialektico.com/introduccion-preprocesamiento/)** de datos, la cual corresponde a la de **limpieza** de estos.\n",
        "\n",
        "Dado lo que hemos mostrado hasta el momento, y lo que sabemos que será necesario analizar, estableceremos la siguiente **lista de tareas** a ejecutar:\n",
        "\n",
        "*   Renombramiento de columnas.\n",
        "*   Eliminación de columnas no solicitadas.\n",
        "*   Remoción de valores nulos.\n",
        "*   Búsqueda y eliminación de datos repetidos.\n",
        "*   Búsqueda y eliminación de valores atípicos."
      ],
      "metadata": {
        "id": "n5C4cdJU4NZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "3Yol2oNaSBy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Eliminación y renombramiento de columnas**"
      ],
      "metadata": {
        "id": "6-BqfYL2z4un"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como se ha señalado antes, se nos ha solicitado utilizar solamente **6 atributos** de nuestro conjunto de datos (tipo de motor, número de cilindros, sistema de combustible, caballos de fuerza, y precio), por lo que nos enfocaremos en matener solo estas columnas.\n",
        "\n",
        "Primero procederemos a identificar las columnas y cambiar sus nombres utilizando la función **[rename()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html)** de Pandas; esto con el fin de tener nombres de variables en español para un análisis y visualización más **cómoda** y **consistente** (y porque el requerimiento ha sido abordado en este idioma):"
      ],
      "metadata": {
        "id": "DauE3Gco-Wov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se renombran las columnas solicitadas en el requerimiento.\n",
        "dataset = dataset.rename(columns = {'engine-type': 'Tipo_de_motor','num-of-cylinders': 'Número_de_cilindros', 'engine-size': 'Tamaño_del_motor',\n",
        "                                    'fuel-system': 'Sistema_de_combustible', 'horsepower': 'Caballos_de_fuerza', 'price': 'Precio'})\n",
        "# Se imprime el conjunto de datos.\n",
        "dataset"
      ],
      "metadata": {
        "id": "sZGQCv0o2oF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora extraemos las columnas solicitadas:"
      ],
      "metadata": {
        "id": "Gj7hu6z-3UUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se mantienen solo las columnas con las características deseadas, y se imprime el resultado.\n",
        "dataset = dataset.loc[:, ['Tipo_de_motor', 'Número_de_cilindros', 'Sistema_de_combustible', 'Tamaño_del_motor', 'Caballos_de_fuerza', 'Precio']]\n",
        "dataset"
      ],
      "metadata": {
        "id": "P7bCBlLPcAIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "HUBiMqT6R_sk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Remoción de valores nulos (NaN)**"
      ],
      "metadata": {
        "id": "QF7bUP5l-d_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Habiendo manipulado los datos para la extracción y edición de algunas **columnas** del conjunto de datos, volvemos a imprimir el resumen de nuestra tabla, con el fin de observar los **datos nulos** remanentes:"
      ],
      "metadata": {
        "id": "eahKhOJbzXPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.info()"
      ],
      "metadata": {
        "id": "p8GFIbCOyrt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como se observa en la salida de esta casilla, ya contamos con solo 6 columnas de entre las cuales 2 cuentan con **valores nulos**.\n",
        "\n",
        "Existen varias formas de encarar la presencia de este tipo de datos, en nuestro caso, dado que son pocos (aproximadamente, el 3% del total de los datos), procederemos simplemente a eliminar los renglones que los contienen utilizando la función **[dropna() ](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html)**:"
      ],
      "metadata": {
        "id": "-S9OHhA20J-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se eliminan los renglones con valores nulos, y se resetea el índice (primera columna con la numeración) para tener una enumeración correcta.\n",
        "dataset = dataset.dropna().reset_index(drop = True)\n",
        "\n",
        "# Se imprime el conjunto de datos.\n",
        "dataset"
      ],
      "metadata": {
        "id": "se28Ovl80FTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notarás que los renglones ahora son 199 (información debajo de la tabla), por lo tanto, se han eliminado 6, lo cual no representa un monto **considerable** de **ejemplos de entrenamiento**.\n"
      ],
      "metadata": {
        "id": "M0XKH-Dq26E2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nota:** el tratamiento de valores nulos o valores faltantes es todo un tema a explorar, y para el cual se deben considerar distintos métodos y casos. Esto lo veremos en una lección futura."
      ],
      "metadata": {
        "id": "-4P_x7Ejoy_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "QqpGY8DeR7RH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Búsqueda y eliminación de datos duplicados**"
      ],
      "metadata": {
        "id": "Lu_wWIlq_TR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La existencia de datos (renglones) duplicados suele ser recurrente en conjuntos de datos, por lo que debe ser rutinaria la comprobación de su existencia, y su posterior eliminación, la cual haremos a continuación utilizando la función **[drop_duplicates()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html)** de Pandas.\n",
        "\n",
        "El siguiente bloque imprimirá el número de valores duplicados en todo el conjunto de datos:"
      ],
      "metadata": {
        "id": "WSvgOdTRSvZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se imprime el número de datos duplicados.\n",
        "print('El número de renglones duplicados en el conjunto de datos es: ' + str(len(dataset)-len(dataset.drop_duplicates())))"
      ],
      "metadata": {
        "id": "AeFDX1QJ_odK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al tratarse de pocos valores (aprox. el 3.5% del total), los eliminamos invocando la función indicada:"
      ],
      "metadata": {
        "id": "dMpfkBJxAVY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se eliminan los renglones repetidos y se resetea el índice de nuevo.\n",
        "dataset.drop_duplicates(inplace = True)\n",
        "dataset = dataset.reset_index(drop = True)\n",
        "\n",
        "# Se imprime el conjunto de datos.\n",
        "dataset"
      ],
      "metadata": {
        "id": "UnF3qY1E7asO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "mVPF824BSmjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Remoción de valores atípicos (outliers)**"
      ],
      "metadata": {
        "id": "r4xTtDrqTfM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El último paso a llevar a cabo en nuestra limpieza de datos es la **eliminación** de **datos atípicos**, para la cual existe todo un abanico de métodos, entre los cuales se incluyen incluso algoritmos de **[aprendizaje de máquinas](https://dialektico.com/introduccion-machine-learning/)**. En esta ocasión, en pos de la simplicidad, utilizaremos un método estadístico clásico llamado **rango intercuartílico**, el cuál es una técnica convencional que puede ser utilizada para la eliminación de datos atípicos en conjuntos con **distribuciones no gaussianas.**"
      ],
      "metadata": {
        "id": "AiXRAUvuA1Cn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " En exploraciones futuras, ahondaremos en la inusitada cantidad de métodos disponibles para abordar esta problemática."
      ],
      "metadata": {
        "id": "lRucdGNMmBLD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para una rápida ilustración de lo que representan los **valores atípicos**, graficaremos los datos de los caballos de fuerza de cada coche respecto a su precio, utilizando la librería **[Matplotlib](https://matplotlib.org/stable/index.html)**:"
      ],
      "metadata": {
        "id": "4bmcO5IH4tUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se determinan los datos a graficar, en este caso: el precio y los caballos de fuerza de cada automóvil.\n",
        "x_points = dataset['Caballos_de_fuerza']\n",
        "y_points = dataset['Precio']\n",
        "\n",
        "# Se grafican.\n",
        "plt.figure(figsize=(9, 9))\n",
        "plt.plot(x_points, y_points, 'o',  markersize = 7)\n",
        "plt.title(\"PRECIO DE COCHES RESPECTO A CABALLOS DE FUERZA\", fontdict = {'family': 'DejaVu Sans', 'color':  'black', 'weight': 'bold', 'size': 16})\n",
        "plt.suptitle(\"Fig. 1 Visualización de la dispersión de los datos de dos atributos (caballos de fuerza respecto al precio). En la parte superior, se pueden notar puntos considerablemente alejados de los demás.\",\n",
        "             fontproperties = {'family': 'DejaVu Sans', 'size': 16}, y=-0.001)\n",
        "plt.xlabel(\"CABALLOS DE FUERZA\", fontdict = {'family': 'DejaVu Sans', 'color':  'black', 'weight': 'bold', 'size': 12})\n",
        "plt.ylabel(\"PRECIO\", fontdict = {'family': 'DejaVu Sans', 'color':  'black', 'weight': 'bold', 'size': 12})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S1N8Ny-gwlOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la **Figura 1** se observa cómo hay puntos que parecen alejarse demasiado de aquellos que están más agrupados; estos son los datos que procederemos a eliminar, ya que se consideran **anomalías** que pueden afectar la precisión de los modelos entrenados con esta información poco significativa.\n",
        "\n",
        "Nótese cómo el uso de esta gráfica es parte del análisis exploratorio, más específicamente, de la etapa que implica **visualización de datos**."
      ],
      "metadata": {
        "id": "NnVxPNkA488Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, se eliminan los **valores atípico**s de la variable **Precio**, y se muestra la gráfica después de aplicar esta remoción. Como se señaló antes, la limpieza de estos se realizó con base en un análisis de **rango intercuartílico**, el cual no detallaremos por el momento, ya que el fin de esta práctica es la demostración de los resultados del preprocesamiento de datos:"
      ],
      "metadata": {
        "id": "kdpNa7rF8RTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se crea una función que calcula los límites superior e inferior del rango intercuartílico.\n",
        "def iqr_filter(column):\n",
        "  percentile25 = dataset[column].quantile(0.25)\n",
        "  percentile75 = dataset[column].quantile(0.75)\n",
        "  iqr = percentile75 - percentile25\n",
        "\n",
        "  upper_limit = percentile75 + 1.5 * iqr\n",
        "  lower_limit = percentile25 - 1.5 * iqr\n",
        "\n",
        "  return upper_limit,lower_limit\n",
        "\n",
        "# Se crea un conjunto de datos temporal para hacer las comparaciones.\n",
        "temp_dataset = dataset\n",
        "\n",
        "# Se realiza la detección y eliminación de los valores atípicos en los precios.\n",
        "upper_p,lower_p = iqr_filter('Precio')\n",
        "temp_dataset = temp_dataset[dataset['Precio'] > lower_p]\n",
        "temp_dataset = temp_dataset[dataset['Precio'] < upper_p]\n",
        "\n",
        "x_points = dataset['Caballos_de_fuerza'] # Puntos del conjunto de datos original\n",
        "y_points = dataset['Precio']\n",
        "\n",
        "t_xpoints = temp_dataset['Caballos_de_fuerza'] # Puntos del conjunto de datos temporal.\n",
        "t_ypoints = temp_dataset['Precio']\n",
        "\n",
        "# Se declaran un conjunto de datos con los outliers, para la ilustración de su detección y elminación.\n",
        "out_dataset = pd.concat([dataset,temp_dataset]).drop_duplicates(keep=False)\n",
        "\n",
        "o_xpoints = out_dataset['Caballos_de_fuerza'] # Puntos del conjunto de datos atípicos.\n",
        "o_ypoints = out_dataset['Precio']\n",
        "\n",
        "# Se despliegan las gráficas.\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 12), sharex=True)\n",
        "\n",
        "ax1.plot(x_points, y_points, 'o')\n",
        "ax1.set_title(\"ANTES DE LA REMOCIÓN\", fontdict = {'family': 'DejaVu Sans', 'color':  'black', 'weight': 'bold', 'size': 14}, pad = 15)\n",
        "ax1.set_xlabel(\"CABALLOS DE FUERZA\", fontdict = {'family': 'DejaVu Sans', 'color':  'black', 'weight': 'bold', 'size': 10}, labelpad=15)\n",
        "ax1.set_ylabel(\"PRECIO\", fontdict = {'family': 'DejaVu Sans', 'color':  'black', 'weight': 'bold', 'size': 10}, labelpad=15)\n",
        "\n",
        "ax2.scatter(x_points, y_points)\n",
        "ax2.scatter(o_xpoints, o_ypoints, c = 'red', label = 'Valores atípicos')\n",
        "ax2.set_title(\"DATOS ATÍPICOS DETECTADOS\", fontdict = {'family': 'DejaVu Sans', 'color':  'black', 'weight': 'bold', 'size': 14}, pad = 15)\n",
        "ax2.set_xlabel(\"CABALLOS DE FUERZA\", fontdict = {'family': 'DejaVu Sans', 'color':  'black', 'weight': 'bold', 'size': 10}, labelpad=15)\n",
        "ax2.set_ylabel(\"PRECIO\", fontdict = {'family': 'DejaVu Sans', 'color':  'black', 'weight': 'bold', 'size': 10})\n",
        "ax2.legend(loc='upper left', prop = {'family': 'DejaVu Sans', 'weight': 'bold', 'size': 14}, frameon = True, framealpha = 1, facecolor  = '#dddddd', shadow = True)\n",
        "\n",
        "ax3.scatter(x_points, y_points)\n",
        "ax3.scatter(o_xpoints, o_ypoints, s= 120, c = 'white')\n",
        "ax3.set_title(\"DESPUÉS DE LA REMOCIÓN\", fontdict = {'family': 'DejaVu Sans', 'color':  'black', 'weight': 'bold', 'size': 14}, pad = 15)\n",
        "ax3.set_xlabel(\"CABALLOS DE FUERZA\", fontdict = {'family': 'DejaVu Sans', 'color':  'black', 'weight': 'bold', 'size': 10}, labelpad=15)\n",
        "ax3.set_ylabel(\"PRECIO\", fontdict = {'family': 'DejaVu Sans', 'color':  'black', 'weight': 'bold', 'size': 10})\n",
        "\n",
        "plt.suptitle(\"Fig. 2 Gráficas comparativas de datos antes (izquierda) y después (derecha) de la remoción de valores atípicos. En medio se pueden apreciar los puntos (en color rojo) que se han determinado como anómalos.\",\n",
        "             fontproperties = {'family': 'DejaVu Sans', 'size': 16}, y=-0.001)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g3ZOO8gq3ptC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la **Figura 2** se ilustra cómo cambia la **dispersión** de los datos antes y después de la **eliminación** de las **observaciones atípicas**, dando como resultado (tercera gráfica) puntos más cercanos entre sí.\n",
        "\n",
        "Habiendo vislumbrado el objetivo de esta técnica, se procede a hacer lo mismo para todas las columnas a excepción de aquellas cuyos valores no son **numéricos** (solo las primeras tres columnas):"
      ],
      "metadata": {
        "id": "3qfPRoeYD_Ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se ejecuta la eliminación de outliers con la función antes definida.\n",
        "for i in dataset.columns[3:].tolist():\n",
        "  upper_p,lower_p = iqr_filter(i)\n",
        "  dataset = dataset.loc[dataset[i] > lower_p]\n",
        "  dataset = dataset.loc[dataset[i] < upper_p]\n",
        "\n",
        "# Se reinicia el índice y se imprime la tabla resultante.\n",
        "dataset = dataset.reset_index(drop = True)\n",
        "dataset"
      ],
      "metadata": {
        "id": "QSCpUymSEnD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta última tabla se observa un total de **175 renglones**, por lo que el conjunto de datos fue limpiado a una versión más **estadísticamente** conveniente para nuestros fines.\n",
        "\n",
        "**Nota**: el número de métodos para la detección de outliers es extenso, y varía en función de las características de cada conjunto de datos. Entre las librerías que se pueden emplear para estos fines se encuentran [PyOD](https://github.com/yzhao062/pyod), [Alibi Detect](https://github.com/SeldonIO/alibi-detect), y [PyNomaly](https://github.com/vc1492a/PyNomaly), de entre las cuales utilizaremos algunas a lo largo de las prácticas del curso."
      ],
      "metadata": {
        "id": "bOw4Z6bXFnfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://dialektico.com/wp-content/uploads/2024/10/PPD_M7.jpg\" alt=\"Dialéktico Logo\" width=\"600\" /></center>\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "Ck52Vn6GZsPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "fp7_nWNzVOMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformación**\n",
        "Como ya se ha [definido](https://dialektico.com/introduccion-preprocesamiento/#transf), la **transformación** de los datos consiste en expresarlos en formas que posibiliten y/u optimicen su procesamiento computacional.\n",
        "\n",
        "En esta etapa se llevarán a cabo las siguientes actividades:\n",
        "\n",
        "\n",
        "\n",
        "*   Codificación de datos cualitativos a tipo cuantitativo.\n",
        "*   Escalado de datos."
      ],
      "metadata": {
        "id": "JDeJBlVYT5Zd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, se desglosan sus razones y métodos."
      ],
      "metadata": {
        "id": "NDbjmgvxfr2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "JgEz_FRpVexX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Codificación de datos cualitativos**"
      ],
      "metadata": {
        "id": "QUtyAAl9tLZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para aclarar el por qué de este primer paso, revisemos cómo ha quedado la información de nuestro conjunto de datos después de la primera etapa de preprocesamiento:\n"
      ],
      "metadata": {
        "id": "YTM9dF1_t6oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se despliega la información general del conjunto de datos.\n",
        "dataset.info()"
      ],
      "metadata": {
        "id": "-S3OdaUirLZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se puede notar que los **valores nulos** han desaparecido, además de haber disminuido el número de **atributos**, cambiado sus nombres, y reducido el número de ejemplos de entrenamiento.\n",
        "\n",
        "Ahora dirijamos nuestra atención a la sección con el título \"**Dtype**\", la cual nos muestra el tipo de datos que contiene cada columna; en esta notaremos algunos valores de tipo **enteros** (int64) o **racionales** (float64), es decir, de tipo cuantitativo; sin embargo, también notaremos algunas columnas con valores **[cualitativos](https://dialektico.com/datos-machine-learning/#Dcualit)**, como la correspondiente al **tipo de motor**.\n",
        "\n",
        "Dado que en nuestros **objetivos** se ha establecido que estos datos se utilizarán como **entradas** para un modelo de **aprendizaje automático**, los datos de todo el conjunto deberán ser de **tipo numérico**, por lo que procederemos a transformar todos los datos cualitativos en representaciones matemáticas de los mismos.\n"
      ],
      "metadata": {
        "id": "Gln6Kl9Psa5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Para llevar a cabo esta transformación utilizaremos la librería **[Sklearn](https://scikit-learn.org/stable/index.html)** y su clase **[LabelEncoder()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)**, la cual asignará un número entero a cada uno de los posibles valores de la columna.\n",
        "\n",
        "**Nota:** utilizaremos esta técnica para mantener la sencillez de la experiencia del preprocesamiento, mas no significa que sea la mejor opción entre todo el espectro de posibilidades. La forma de generar representaciones numéricas para cada variable dependerá de factores asociados al modelo del cual serán entradas. Esto lo aprenderemos con detalle más adelante."
      ],
      "metadata": {
        "id": "uCCqfZ4qu1Kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observemos cómo se aplica esto a la característica \"**Tipo_de_motor**\".\n",
        "\n",
        "Partiremos observando cuáles son los diferentes **valores** de este atributo:"
      ],
      "metadata": {
        "id": "7nvxuF3GvuR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se imprimen los distintos valores de la columna Tipo_de_motor, y el número de veces que aparece cada uno.\n",
        "dataset['Tipo_de_motor'].value_counts()"
      ],
      "metadata": {
        "id": "dHW69-KivAT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la primer columna se observa el valor que puede tomar el **atributo**, y en la segunda columna el **número de veces** que aparece cada uno. Por ejemplo, hay 131 renglones o ejemplos de entrenamiento en los que aparece \"ohc\" como valor del **tipo de motor**.\n",
        "\n",
        "Sabiendo que existen 6 posibles valores, procedemos a codificarlos numéricamente:"
      ],
      "metadata": {
        "id": "Vre5mOakwF3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se declara el codificador.\n",
        "le = preprocessing.LabelEncoder()\n",
        "\n",
        "# Se transforman los valores de la columna en enteros.\n",
        "dataset['Tipo_de_motor'] = le.fit_transform(dataset['Tipo_de_motor'])\n",
        "\n",
        "# Se imprime el conjunto de datos.\n",
        "dataset"
      ],
      "metadata": {
        "id": "fpf-YEJFcTjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la tabla se puede constatar que ahora los valores para la variable \"Tipo_de_motor\" son **numéricos**: enteros que se han asignado a cada posible dato cualitativo.\n",
        "\n",
        "La transformación se dio de la siguiente forma:\n",
        "\n",
        "\n",
        "*   **dohc** = 0\n",
        "*   **l** = 1\n",
        "*   **ohc** = 2\n",
        "*   **ohcf** = 3\n",
        "*   **ohcv** = 4\n",
        "*   **rotor** = 5\n",
        "\n"
      ],
      "metadata": {
        "id": "sXKrqjZ-w-Ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Después de ver cómo se da esta transformación, procedemos a aplicar la misma a las demás columnas con datos de tipo cualitativo:"
      ],
      "metadata": {
        "id": "ZNMXkCyWjPGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se aplica el codificador a cada columna con valores no numéricos.\n",
        "dataset['Número_de_cilindros'] = le.fit_transform(dataset['Número_de_cilindros'])\n",
        "dataset['Sistema_de_combustible'] = le.fit_transform(dataset['Sistema_de_combustible'])\n",
        "\n",
        "# Se imprime el conjunto de datos.\n",
        "dataset"
      ],
      "metadata": {
        "id": "Bu3YANiWyqpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El resultado es un conjunto de datos con todos los valores de tipo **numéricos**."
      ],
      "metadata": {
        "id": "gPDOC3c0zQoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "44Uw44Kaa20Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Escalado de datos y análisis descriptivo**"
      ],
      "metadata": {
        "id": "reH3KFjrzX0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora que tenemos valores completamente numéricos, procedemos a explorarlos de otra forma: realizando un breve análisis descriptivo desplegando algunas estadísticas básicas utilizando la función **describe()** de Pandas, la cual nos mostrará la siguiente información de los valores de cada columna:\n",
        "\n",
        "\n",
        "\n",
        "*   **Count**: número de valores de cada columna.\n",
        "*   **Mean**: su promedio.\n",
        "*   **Std**: su desviación estándar.\n",
        "*   **Min**: su valor mínimo.\n",
        "*   **Max**: su valor máximo.\n",
        "*   **25%, 50%, 75%**: sus percentiles\n"
      ],
      "metadata": {
        "id": "ParUWmvPzexZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Desplegamos las estadísticas del conjunto de datos.\n",
        "dataset.describe()"
      ],
      "metadata": {
        "id": "QMnQwRPL74fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la tabla puedes observar los valores de cada estadística para cada atributo o columna existente.\n",
        "\n",
        "Por el momento, nos centraremos en los valores de los rangos (mínimos y máximos). Sin contar los datos de aquellas variables que transformamos en el paso anterior, ya que sus valores son poco diversos y todos **discretos**, se pueden discernir cantidades **máximas** y **mínimas** muy distantes entre sí (en un rango de 48 a 25,552).\n",
        "\n",
        "Dato que estos datos fungirán como **entradas** para un **algoritmo** de machine learning, lo recomendable es **escalar** los datos (hacer más pequeñas las distancias entre las observaciones colocándolos en un intervalo) para eficientar su **procesamiento**."
      ],
      "metadata": {
        "id": "GQtPtDi2zzJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para esto recurriremos a la clase **[MinMaxScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)** de Sklearn, la cual escalará los datos a un intervalo de 0 a 1:"
      ],
      "metadata": {
        "id": "G0XPOlqBozwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se declara la clase que escalará nuestros datos.\n",
        "scaler = preprocessing.MinMaxScaler()\n",
        "\n",
        "# Se escalan los datos a un intervalo entre 0 y 1.\n",
        "normalized = scaler.fit_transform(dataset)\n",
        "\n",
        "# Se toman los valores de las columnas y se recrea la tabla con los datos escalados.\n",
        "column_names = dataset.columns\n",
        "dataset = pd.DataFrame(normalized, columns = column_names)\n",
        "\n",
        "# Se imprime el conjunto de datos.\n",
        "dataset"
      ],
      "metadata": {
        "id": "DBR8lv8pnhfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podrás notar, el resultado es el conjunto de datos con los números escalados entre los valores 0 y 1, lo cual minimiza las distancias y procura un mayor **rendimiento** de procesamiento."
      ],
      "metadata": {
        "id": "h-DRgGxDrQrv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "PovdQpZDbz4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Resultados**\n",
        "Hemos terminado nuestro preprocesamiento. Ahora que se ha aplicado limpieza y transformación a nuestros datos, podemos visualizar el antes y el después de estas etapas:"
      ],
      "metadata": {
        "id": "8SdVNPTqtkLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conjunto de datos inicial:**"
      ],
      "metadata": {
        "id": "6WI9zKAhjfkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orig_dataset = pd.read_csv('https://raw.githubusercontent.com/DanielDialektico/dialektico-machine-learning-practices/refs/heads/main/data/Autom%C3%B3viles.csv')\n",
        "orig_dataset"
      ],
      "metadata": {
        "id": "jCTdADObiTzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conjunto de datos preprocesado:**"
      ],
      "metadata": {
        "id": "kQ-1q7U7jr_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "byffijjojvTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notarás que el conjunto de datos cambió considerablemente en cuanto a su **estructura** y **contenido**, y aunque podrían hacerse más adecuaciones dependiendo del objetivo fijado, este ahora está más **optimizado** para alimentar un modelo de machine learning con los atributos solicitados."
      ],
      "metadata": {
        "id": "TrdZyLlQot5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Así concluye nuestra primera **práctica** en Python, el fin de este breve encuentro fue el de mostrar qué tipo de cambios se pueden llegar a necesitar suscitar en los datos para un eficiente procesamiento. Realizamos tareas de análisis descriptivo, inferencial, visualización de datos, y de limpieza y transformación. En esta ocasión, no fueron necesarias etapas de **reducción** o **integración**, pero no olvides que forma parte de las posibles técnicas del **preprocesamiento**.\n",
        "\n",
        "No te preocupes por los detalles de todo lo que hicimos aquí, ha sido meramente demostrativo. El preprocesamiento de datos es una actividad práctamente inexorable en la creación de **modelos** de **aprendizaje de máquinas**, por lo que se irá practicando como una etapa inicial en la mayoría de los algoritmos abordados en el curso.\n",
        "\n",
        "▶ [Regresar a la lección](https://dialektico.com/introduccion-preprocesamiento/#c%C3%B3digo) 🧙"
      ],
      "metadata": {
        "id": "_5WBEo_5iR7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "bFdmIM_TcX6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apéndice\n",
        "\n",
        "## Sobre el conjunto de datos\n",
        "El conjunto de datos utilizado en este ejercicio fue obtenido del Repositorio de Aprendizaje de Máquinas UCI del Centro para Machine Learning e Inteligencia Artificial de la Universidad de California.\n",
        "\n",
        "Estos datos fueron recolectados por Jeffrey C. Schlimmer (Jeffrey.Schlimmer '@' a.gp.cs.cmu.edu), de las siguientes fuentes:\n",
        "\n",
        "*   1985 Model Import Car and Truck Specifications, 1985 Ward's Automotive Yearbook.\n",
        "*   Personal Auto Manuals, Insurance Services Office, 160 Water Street, New York, NY 10038.\n",
        "*Insurance Collision Report, Insurance Institute for Highway Safety, Watergate 600, Washington, DC 20037.\n",
        "\n",
        "**Atributos y sus rangos**:\n",
        "1. **symboling**: -3, -2, -1, 0, 1, 2, 3.\n",
        "2. **normalized-losses**: continuo de 65 a 256.\n",
        "3. **make**:\n",
        "alfa-romero, audi, bmw, chevrolet, dodge, honda,\n",
        "isuzu, jaguar, mazda, mercedes-benz, mercury,\n",
        "mitsubishi, nissan, peugot, plymouth, porsche,\n",
        "renault, saab, subaru, toyota, volkswagen, volvo\n",
        "\n",
        "4. **fuel-type**: diesel, gasolina.\n",
        "5. **aspiration**: std, turbo.\n",
        "6. **num-of-doors**: cuatro, dos.\n",
        "7. **body-style**:: techo duro, wagon, sedán, hatchback, descapotable.\n",
        "8. **drive-wheels**: 4x4, fwd, rwd.\n",
        "9. **engine-location**: delantero, trasero.\n",
        "10. **wheel-base**: continua de 86.6 a 120.9.\n",
        "11. **lenght**: continua de 141.1 a 208.1.\n",
        "12. **width**: continua de 60.3 a 72.3.\n",
        "13. **height**: continua de 47.8 a 59.8.\n",
        "14. **curb-weigh**: continuo de 1488 a 4066.\n",
        "15. **engine-type**: dohc, dohcv, l, ohc, ohcf, ohcv, rotor.\n",
        "16. **num-of-cylinders**: ocho, cinco, cuatro, seis, tres, doce, dos.\n",
        "17. **engine-size**: continuo de 61 a 326.\n",
        "18. **fuel-system**: 1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi.\n",
        "19. **bore**: continuo de 2.54 a 3.94.\n",
        "20. **stroke**: continua de 2.07 a 4.17.\n",
        "21. **compression-ratio**: continua de 7 a 23.\n",
        "22. **horsepower**: continua de 48 a 288.\n",
        "23. **peak-rpm**: continua de 4.150 a 6.600 rpm.\n",
        "24. **city-mpg**: continuo de 13 a 49.\n",
        "25. **highway-mpg**: continuo de 16 a 54."
      ],
      "metadata": {
        "id": "HaVTloDvHZTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MIT License\n",
        "#\n",
        "# Copyright (c) 2023 Daniel García\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ],
      "metadata": {
        "id": "APuJgH91GaF7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}