{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNN382JjnCuiVgL78H3j0ZU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielDialektico/dialektico-machine-learning-practices/blob/main/notebooks/Preprocesamiento/Preprocesamiento_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://dialektico.com/wp-content/uploads/2023/03/MiniLogoW4.png\" alt=\"Dial√©ktico Logo\" />"
      ],
      "metadata": {
        "id": "M_JD72l23F97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**AN√ÅLISIS EXPLORATORIO Y PREPROCESAMIENTO DE DATOS CON PYTHON** ‚öí\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tV0QYCqa0vT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introducci√≥n**\n",
        "El fin de esta pr√°ctica es darte un recorrido en el que observar√°s c√≥mo se **analiza** y **preprocesa** un conjunto de datos con algunas librer√≠as de **Python** (principalmente **Pandas**, la cual nos permite trabajar de forma sencilla con informaci√≥n dispuesta en tablas), con la pretensi√≥n de que experimentes las distintas etapas de edici√≥n por las que puede pasar un conjunto de datos antes de llegar a alimentar un modelo de **machine learning**.\n",
        "\n",
        "Ejecutar√°s c√≥digo que te ayude a **analizar**, **limpiar** y **transformar** un conjunto de datos para generar uno m√°s comprensible y digerible para la m√°quina, y/o para quien lo manipule y utilic√© con alg√∫n fin anal√≠tico.\n",
        "\n",
        "  <center><img src=\"https://dialektico.com/wp-content/uploads/2024/05/Prepro_Eq.jpg\" width=\"300\" /></center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ShK24xsfCnBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "KWYvkDJ2SG4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Objetivo**\n",
        "Para esta pr√°ctica, se plantea el siguiente requerimiento:\n",
        "\n",
        "*  Se nos ha dado la tarea de **analizar** un conjunto de datos de autom√≥viles para hallar **insights** (descubrimientos relevantes), y posteriormente crear un modelo de **machine learning** que ser√° utilizado por el √°rea de marketing. Sin embargo, sabemos que antes debemos de ejecutar una etapa de **preprocesamiento** para llevar a cabo estas tareas de manera eficiente, lo cual, a su vez, implica un an√°lisis exploratorio de datos.\n",
        "*  Nuestro objetivo ser√° la ejecuci√≥n de esta etapa previa, realizando los cambios necesarios conforme a la informaci√≥n provista por el an√°lisis.\n",
        "*  El conjunto de datos a trabajar es una tabla con **256 ejemplos de entrenamiento**, los cuales contienen **26 caracter√≠sticas** de cada coche.\n",
        "*  Para completar nuestra tarea, el equipo de marketing nos ha pedido que solo utilicemos los siguientes **atributos** (columnas): tipo de motor, n√∫mero de cilindros, sistema de combustible, caballos de fuerza, y precio.\n",
        "\n",
        "<br>\n",
        "<center><img src=\"https://dialektico.com/wp-content/uploads/2024/10/PDD_Colab_png_2.png\" alt=\"Dial√©ktico Logo\" width=\"400\" /></center>"
      ],
      "metadata": {
        "id": "KqMzw_24Ey-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nota**: el conjunto de datos que utilizaremos comprende **informaci√≥n real** de acceso p√∫blico. Para m√°s informaci√≥n, cons√∫ltese el **[ap√©ndice](https://colab.research.google.com/drive/10O1A-C6nKCfaCrb1Vu8ayfDuBtIIzE_9#scrollTo=HaVTloDvHZTr)** al final de la lecci√≥n."
      ],
      "metadata": {
        "id": "KrUL9MMjv6m4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "cbhY8ixRSNws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Declaraci√≥n de librer√≠as y carga de datos**\n",
        "En este caso, el conjunto de datos se descargar√° desde un repositorio en **[GitHub](https://github.com/DanielDialektico)**. Se trata de una sola tabla, por lo que no ser√° necesario acudir a una etapa de **integraci√≥n** del **preprocesamiento** de datos.\n",
        "\n",
        "Para cargar el conjunto de datos a nuestro espacio de trabajo, se extraer√° la informaci√≥n (alojada en formato CSV) utilizando la URL del archivo, y se almacenar√° en una **tabla** (dataFrame) utilizando la librer√≠a **Pandas**.\n",
        "\n",
        "Antes que nada, se importar√°n las **librer√≠as** a utilizar, lo cual encontrar√°s al principio de la siguiente celda de c√≥digo."
      ],
      "metadata": {
        "id": "v_xCEZn-QSwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://dialektico.com/wp-content/uploads/2024/10/PPD_M6.jpg\" alt=\"Dial√©ktico Logo\" width=\"600\" /></center>\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "Wq_2Fz9eSXRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecuta la siguiente celda para cargar los datos y las librer√≠as, y de ahora en adelante, haz lo mismo con todos los bloques conforme vayan apareciendo:"
      ],
      "metadata": {
        "id": "Zf7_0kfdLeHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se importan las librer√≠as.\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn import preprocessing\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "# Se define el tipo de formato de las gr√°ficas a utilizar (se utilizar√°n m√°s adelante).\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "# Se ignoran las alertas.\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Se carga el conjunto de datos desde GitHub\n",
        "dataset = pd.read_csv('https://raw.githubusercontent.com/DanielDialektico/dialektico-machine-learning-practices/refs/heads/main/data/Autom%C3%B3viles.csv')"
      ],
      "metadata": {
        "id": "ILh0ohbvSlQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprobamos que la carga se haya realizado correctamente mostrando los primeros cinco renglones de la tabla:"
      ],
      "metadata": {
        "id": "4BraKNkeT9iC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se especifica que se muestren todas las columnas de la tabla.\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Se imprimen los primeros cinco renglones.\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "CWbE5EpdUG1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "kdU2Fq7nSWkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **An√°lisis inicial y limpieza de datos**\n",
        "Ahora que hemos cargado nuestro conjunto de datos, realizaremos una exploraci√≥n inicial de los datos, para lo cual visualizaremos las caracter√≠sticas generales del conjunto.\n",
        "\n",
        "Primero, utilizaremos la funcion **[info()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html)** de Pandas, que imprimir√° la siguiente informaci√≥n:  \n",
        "\n",
        "* **RangeIndex**: N√∫mero de renglones.\n",
        "* **Data Columns**: N√∫mero de columnas y caracter√≠sticas de columnas:\n",
        "    * **Column**: el nombre de cada columna.\n",
        "    * **Non-Null Count**: el n√∫mero de valores no nulos, los cuales son los datos v√°lidos, es decir, datos no vac√≠os o que no generan alg√∫n conflicto de lectura.\n",
        "    * **Dtype**: el tipo de datos en el que est√° categorizada cada columna.\n"
      ],
      "metadata": {
        "id": "4dbnsCpLPcZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Ejecuta la siguiente celda para visualizar esta informaci√≥n:"
      ],
      "metadata": {
        "id": "KrGiLGq0brZm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZLfS1_ihwKQ"
      },
      "outputs": [],
      "source": [
        "# Se despliega la informaci√≥n general del conjunto de datos.\n",
        "dataset.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta lista nos podemos percatar de que tenemos **26 columnas y 205 renglones**, y de que en algunas columnas tenemos la presencia de **valores nulos**, ya que el conteo \"non-null\" no siempre es igual a 205.\n",
        "\n",
        "El ejemplo m√°s claro de esto es el de la columna \"**normalized-losses**\", la cual cuenta solo con 164 valores no nulos, indicando un total de 41 valores de este tipo presentes.\n"
      ],
      "metadata": {
        "id": "ozEqAzr7hAMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con esta informaci√≥n, y las instrucciones dadas en el planteamiento del **objetivo**, podemos comenzar la primera etapa del **[preprocesamiento](https://dialektico.com/introduccion-preprocesamiento/)** de datos, la cual corresponde a la de **limpieza** de estos.\n",
        "\n",
        "Dado lo que hemos mostrado hasta el momento, y lo que sabemos que ser√° necesario analizar, estableceremos la siguiente **lista de tareas** a ejecutar:\n",
        "\n",
        "*   Renombramiento de columnas.\n",
        "*   Eliminaci√≥n de columnas no solicitadas.\n",
        "*   Remoci√≥n de valores nulos.\n",
        "*   B√∫squeda y eliminaci√≥n de datos repetidos.\n",
        "*   B√∫squeda y eliminaci√≥n de valores at√≠picos."
      ],
      "metadata": {
        "id": "n5C4cdJU4NZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "3Yol2oNaSBy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Eliminaci√≥n y renombramiento de columnas**"
      ],
      "metadata": {
        "id": "6-BqfYL2z4un"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como se ha se√±alado antes, se nos ha solicitado utilizar solamente **6 atributos** de nuestro conjunto de datos (tipo de motor, n√∫mero de cilindros, sistema de combustible, caballos de fuerza, y precio), por lo que nos enfocaremos en matener solo estas columnas.\n",
        "\n",
        "Primero procederemos a identificar las columnas y cambiar sus nombres utilizando la funci√≥n **[rename()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html)** de Pandas; esto con el fin de tener nombres de variables en espa√±ol para un an√°lisis y visualizaci√≥n m√°s **c√≥moda** y **consistente** (y porque el requerimiento ha sido abordado en este idioma):"
      ],
      "metadata": {
        "id": "DauE3Gco-Wov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se renombran las columnas solicitadas en el requerimiento.\n",
        "dataset = dataset.rename(columns = {'engine-type': 'Tipo_de_motor','num-of-cylinders': 'N√∫mero_de_cilindros', 'engine-size': 'Tama√±o_del_motor',\n",
        "                                    'fuel-system': 'Sistema_de_combustible', 'horsepower': 'Caballos_de_fuerza', 'price': 'Precio'})\n",
        "# Se imprime el conjunto de datos.\n",
        "dataset"
      ],
      "metadata": {
        "id": "sZGQCv0o2oF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora extraemos las columnas solicitadas:"
      ],
      "metadata": {
        "id": "Gj7hu6z-3UUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se mantienen solo las columnas con las caracter√≠sticas deseadas, y se imprime el resultado.\n",
        "dataset = dataset.loc[:, ['Tipo_de_motor', 'N√∫mero_de_cilindros', 'Sistema_de_combustible', 'Tama√±o_del_motor', 'Caballos_de_fuerza', 'Precio']]\n",
        "dataset"
      ],
      "metadata": {
        "id": "P7bCBlLPcAIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "HUBiMqT6R_sk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Remoci√≥n de valores nulos (NaN)**"
      ],
      "metadata": {
        "id": "QF7bUP5l-d_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Habiendo manipulado los datos para la extracci√≥n y edici√≥n de algunas **columnas** del conjunto de datos, volvemos a imprimir el resumen de nuestra tabla, con el fin de observar los **datos nulos** remanentes:"
      ],
      "metadata": {
        "id": "eahKhOJbzXPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.info()"
      ],
      "metadata": {
        "id": "p8GFIbCOyrt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como se observa en la salida de esta casilla, ya contamos con solo 6 columnas de entre las cuales 2 cuentan con **valores nulos**.\n",
        "\n",
        "Existen varias formas de encarar la presencia de este tipo de datos, en nuestro caso, dado que son pocos (aproximadamente, el 3% del total de los datos), procederemos simplemente a eliminar los renglones que los contienen utilizando la funci√≥n **[dropna() ](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html)**:"
      ],
      "metadata": {
        "id": "-S9OHhA20J-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se eliminan los renglones con valores nulos, y se resetea el √≠ndice (primera columna con la numeraci√≥n) para tener una enumeraci√≥n correcta.\n",
        "dataset = dataset.dropna().reset_index(drop = True)\n",
        "\n",
        "# Se imprime el conjunto de datos.\n",
        "dataset"
      ],
      "metadata": {
        "id": "se28Ovl80FTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notar√°s que los renglones ahora son 199 (informaci√≥n debajo de la tabla), por lo tanto, se han eliminado 6, lo cual no representa un monto **considerable** de **ejemplos de entrenamiento**.\n"
      ],
      "metadata": {
        "id": "M0XKH-Dq26E2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nota:** el tratamiento de valores nulos o valores faltantes es todo un tema a explorar, y para el cual se deben considerar distintos m√©todos y casos. Esto lo veremos en una lecci√≥n futura."
      ],
      "metadata": {
        "id": "-4P_x7Ejoy_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "QqpGY8DeR7RH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **B√∫squeda y eliminaci√≥n de datos duplicados**"
      ],
      "metadata": {
        "id": "Lu_wWIlq_TR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La existencia de datos (renglones) duplicados suele ser recurrente en conjuntos de datos, por lo que debe ser rutinaria la comprobaci√≥n de su existencia, y su posterior eliminaci√≥n, la cual haremos a continuaci√≥n utilizando la funci√≥n **[drop_duplicates()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html)** de Pandas.\n",
        "\n",
        "El siguiente bloque imprimir√° el n√∫mero de valores duplicados en todo el conjunto de datos:"
      ],
      "metadata": {
        "id": "WSvgOdTRSvZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se imprime el n√∫mero de datos duplicados.\n",
        "print('El n√∫mero de renglones duplicados en el conjunto de datos es: ' + str(len(dataset)-len(dataset.drop_duplicates())))"
      ],
      "metadata": {
        "id": "AeFDX1QJ_odK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al tratarse de pocos valores (aprox. el 3.5% del total), los eliminamos invocando la funci√≥n indicada:"
      ],
      "metadata": {
        "id": "dMpfkBJxAVY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se eliminan los renglones repetidos y se resetea el √≠ndice de nuevo.\n",
        "dataset.drop_duplicates(inplace = True)\n",
        "dataset = dataset.reset_index(drop = True)\n",
        "\n",
        "# Se imprime el conjunto de datos.\n",
        "dataset"
      ],
      "metadata": {
        "id": "UnF3qY1E7asO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "mVPF824BSmjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Remoci√≥n de valores at√≠picos (outliers)**"
      ],
      "metadata": {
        "id": "r4xTtDrqTfM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El √∫ltimo paso a llevar a cabo en nuestra limpieza de datos es la **eliminaci√≥n** de **datos at√≠picos**, para la cual existe todo un abanico de m√©todos, entre los cuales se incluyen incluso algoritmos de **[aprendizaje de m√°quinas](https://dialektico.com/introduccion-machine-learning/)**. En esta ocasi√≥n, en pos de la simplicidad, utilizaremos un m√©todo estad√≠stico cl√°sico llamado **rango intercuart√≠lico**, el cu√°l es una t√©cnica convencional que puede ser utilizada para la eliminaci√≥n de datos at√≠picos en conjuntos con **distribuciones no gaussianas.**"
      ],
      "metadata": {
        "id": "AiXRAUvuA1Cn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " En exploraciones futuras, ahondaremos en la inusitada cantidad de m√©todos disponibles para abordar esta problem√°tica."
      ],
      "metadata": {
        "id": "lRucdGNMmBLD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para una r√°pida ilustraci√≥n de lo que representan los **valores at√≠picos**, graficaremos los datos de los caballos de fuerza de cada coche respecto a su precio, utilizando la librer√≠a **[Matplotlib](https://matplotlib.org/stable/index.html)**:"
      ],
      "metadata": {
        "id": "4bmcO5IH4tUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se determinan los datos a graficar, en este caso: el precio y los caballos de fuerza de cada autom√≥vil.\n",
        "x_points = dataset['Caballos_de_fuerza']\n",
        "y_points = dataset['Precio']\n",
        "\n",
        "# Se grafican.\n",
        "plt.figure(figsize=(9, 9))\n",
        "plt.plot(x_points, y_points, 'o',  markersize = 7)\n",
        "plt.title(\"PRECIO DE COCHES RESPECTO A CABALLOS DE FUERZA\", fontdict = {'family': 'DejaVu Sans', 'color':  'black', 'weight': 'bold', 'size': 16})\n",
        "plt.suptitle(\"Fig. 1 Visualizaci√≥n de la dispersi√≥n de los datos de dos atributos (caballos de fuerza respecto al precio). En la parte superior, se pueden notar puntos considerablemente alejados de los dem√°s.\",\n",
        "             fontproperties = {'family': 'DejaVu Sans', 'size': 16}, y=-0.001)\n",
        "plt.xlabel(\"CABALLOS DE FUERZA\", fontdict = {'family': 'DejaVu Sans', 'color':  'black', 'weight': 'bold', 'size': 12})\n",
        "plt.ylabel(\"PRECIO\", fontdict = {'family': 'DejaVu Sans', 'color':  'black', 'weight': 'bold', 'size': 12})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S1N8Ny-gwlOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la **Figura 1** se observa c√≥mo hay puntos que parecen alejarse demasiado de aquellos que est√°n m√°s agrupados; estos son los datos que procederemos a eliminar, ya que se consideran **anomal√≠as** que pueden afectar la precisi√≥n de los modelos entrenados con esta informaci√≥n poco significativa.\n",
        "\n",
        "N√≥tese c√≥mo el uso de esta gr√°fica es parte del an√°lisis exploratorio, m√°s espec√≠ficamente, de la etapa que implica **visualizaci√≥n de datos**."
      ],
      "metadata": {
        "id": "NnVxPNkA488Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuaci√≥n, se eliminan los **valores at√≠pico**s de la variable **Precio**, y se muestra la gr√°fica despu√©s de aplicar esta remoci√≥n. Como se se√±al√≥ antes, la limpieza de estos se realiz√≥ con base en un an√°lisis de **rango intercuart√≠lico**, el cual no detallaremos por el momento, ya que el fin de esta pr√°ctica es la demostraci√≥n de los resultados del preprocesamiento de datos:"
      ],
      "metadata": {
        "id": "kdpNa7rF8RTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se crea una funci√≥n que calcula los l√≠mites superior e inferior del rango intercuart√≠lico.\n",
        "def iqr_filter(column):\n",
        "  percentile25 = dataset[column].quantile(0.25)\n",
        "  percentile75 = dataset[column].quantile(0.75)\n",
        "  iqr = percentile75 - percentile25\n",
        "\n",
        "  upper_limit = percentile75 + 1.5 * iqr\n",
        "  lower_limit = percentile25 - 1.5 * iqr\n",
        "\n",
        "  return upper_limit,lower_limit\n",
        "\n",
        "# Se crea un conjunto de datos temporal para hacer las comparaciones.\n",
        "temp_dataset = dataset\n",
        "\n",
        "# Se realiza la detecci√≥n y eliminaci√≥n de los valores at√≠picos en los precios.\n",
        "upper_p,lower_p = iqr_filter('Precio')\n",
        "temp_dataset = temp_dataset[dataset['Precio'] > lower_p]\n",
        "temp_dataset = temp_dataset[dataset['Precio'] < upper_p]\n",
        "\n",
        "x_points = dataset['Caballos_de_fuerza'] # Puntos del conjunto de datos original\n",
        "y_points = dataset['Precio']\n",
        "\n",
        "t_xpoints = temp_dataset['Caballos_de_fuerza'] # Puntos del conjunto de datos temporal.\n",
        "t_ypoints = temp_dataset['Precio']\n",
        "\n",
        "# Se declaran un conjunto de datos con los outliers, para la ilustraci√≥n de su detecci√≥n y elminaci√≥n.\n",
        "out_dataset = pd.concat([dataset,temp_dataset]).drop_duplicates(keep=False)\n",
        "\n",
        "o_xpoints = out_dataset['Caballos_de_fuerza'] # Puntos del conjunto de datos at√≠picos.\n",
        "o_ypoints = out_dataset['Precio']\n",
        "\n",
        "# Se despliegan las gr√°ficas.\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 12), sharex=True)\n",
        "\n",
        "ax1.plot(x_points, y_points, 'o')\n",
        "ax1.set_title(\"ANTES DE LA REMOCI√ìN\", fontdict = {'family': 'DejaVu Sans', 'color':  'black', 'weight': 'bold', 'size': 14}, pad = 15)\n",
        "ax1.set_xlabel(\"CABALLOS DE FUERZA\", fontdict = {'family': 'DejaVu Sans', 'color':  'black', 'weight': 'bold', 'size': 10}, labelpad=15)\n",
        "ax1.set_ylabel(\"PRECIO\", fontdict = {'family': 'DejaVu Sans', 'color':  'black', 'weight': 'bold', 'size': 10}, labelpad=15)\n",
        "\n",
        "ax2.scatter(x_points, y_points)\n",
        "ax2.scatter(o_xpoints, o_ypoints, c = 'red', label = 'Valores at√≠picos')\n",
        "ax2.set_title(\"DATOS AT√çPICOS DETECTADOS\", fontdict = {'family': 'DejaVu Sans', 'color':  'black', 'weight': 'bold', 'size': 14}, pad = 15)\n",
        "ax2.set_xlabel(\"CABALLOS DE FUERZA\", fontdict = {'family': 'DejaVu Sans', 'color':  'black', 'weight': 'bold', 'size': 10}, labelpad=15)\n",
        "ax2.set_ylabel(\"PRECIO\", fontdict = {'family': 'DejaVu Sans', 'color':  'black', 'weight': 'bold', 'size': 10})\n",
        "ax2.legend(loc='upper left', prop = {'family': 'DejaVu Sans', 'weight': 'bold', 'size': 14}, frameon = True, framealpha = 1, facecolor  = '#dddddd', shadow = True)\n",
        "\n",
        "ax3.scatter(x_points, y_points)\n",
        "ax3.scatter(o_xpoints, o_ypoints, s= 120, c = 'white')\n",
        "ax3.set_title(\"DESPU√âS DE LA REMOCI√ìN\", fontdict = {'family': 'DejaVu Sans', 'color':  'black', 'weight': 'bold', 'size': 14}, pad = 15)\n",
        "ax3.set_xlabel(\"CABALLOS DE FUERZA\", fontdict = {'family': 'DejaVu Sans', 'color':  'black', 'weight': 'bold', 'size': 10}, labelpad=15)\n",
        "ax3.set_ylabel(\"PRECIO\", fontdict = {'family': 'DejaVu Sans', 'color':  'black', 'weight': 'bold', 'size': 10})\n",
        "\n",
        "plt.suptitle(\"Fig. 2 Gr√°ficas comparativas de datos antes (izquierda) y despu√©s (derecha) de la remoci√≥n de valores at√≠picos. En medio se pueden apreciar los puntos (en color rojo) que se han determinado como an√≥malos.\",\n",
        "             fontproperties = {'family': 'DejaVu Sans', 'size': 16}, y=-0.001)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g3ZOO8gq3ptC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la **Figura 2** se ilustra c√≥mo cambia la **dispersi√≥n** de los datos antes y despu√©s de la **eliminaci√≥n** de las **observaciones at√≠picas**, dando como resultado (tercera gr√°fica) puntos m√°s cercanos entre s√≠.\n",
        "\n",
        "Habiendo vislumbrado el objetivo de esta t√©cnica, se procede a hacer lo mismo para todas las columnas a excepci√≥n de aquellas cuyos valores no son **num√©ricos** (solo las primeras tres columnas):"
      ],
      "metadata": {
        "id": "3qfPRoeYD_Ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se ejecuta la eliminaci√≥n de outliers con la funci√≥n antes definida.\n",
        "for i in dataset.columns[3:].tolist():\n",
        "  upper_p,lower_p = iqr_filter(i)\n",
        "  dataset = dataset.loc[dataset[i] > lower_p]\n",
        "  dataset = dataset.loc[dataset[i] < upper_p]\n",
        "\n",
        "# Se reinicia el √≠ndice y se imprime la tabla resultante.\n",
        "dataset = dataset.reset_index(drop = True)\n",
        "dataset"
      ],
      "metadata": {
        "id": "QSCpUymSEnD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta √∫ltima tabla se observa un total de **175 renglones**, por lo que el conjunto de datos fue limpiado a una versi√≥n m√°s **estad√≠sticamente** conveniente para nuestros fines.\n",
        "\n",
        "**Nota**: el n√∫mero de m√©todos para la detecci√≥n de outliers es extenso, y var√≠a en funci√≥n de las caracter√≠sticas de cada conjunto de datos. Entre las librer√≠as que se pueden emplear para estos fines se encuentran [PyOD](https://github.com/yzhao062/pyod), [Alibi Detect](https://github.com/SeldonIO/alibi-detect), y [PyNomaly](https://github.com/vc1492a/PyNomaly), de entre las cuales utilizaremos algunas a lo largo de las pr√°cticas del curso."
      ],
      "metadata": {
        "id": "bOw4Z6bXFnfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://dialektico.com/wp-content/uploads/2024/10/PPD_M7.jpg\" alt=\"Dial√©ktico Logo\" width=\"600\" /></center>\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "Ck52Vn6GZsPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "fp7_nWNzVOMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformaci√≥n**\n",
        "Como ya se ha [definido](https://dialektico.com/introduccion-preprocesamiento/#transf), la **transformaci√≥n** de los datos consiste en expresarlos en formas que posibiliten y/u optimicen su procesamiento computacional.\n",
        "\n",
        "En esta etapa se llevar√°n a cabo las siguientes actividades:\n",
        "\n",
        "\n",
        "\n",
        "*   Codificaci√≥n de datos cualitativos a tipo cuantitativo.\n",
        "*   Escalado de datos."
      ],
      "metadata": {
        "id": "JDeJBlVYT5Zd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuaci√≥n, se desglosan sus razones y m√©todos."
      ],
      "metadata": {
        "id": "NDbjmgvxfr2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "JgEz_FRpVexX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Codificaci√≥n de datos cualitativos**"
      ],
      "metadata": {
        "id": "QUtyAAl9tLZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para aclarar el por qu√© de este primer paso, revisemos c√≥mo ha quedado la informaci√≥n de nuestro conjunto de datos despu√©s de la primera etapa de preprocesamiento:\n"
      ],
      "metadata": {
        "id": "YTM9dF1_t6oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se despliega la informaci√≥n general del conjunto de datos.\n",
        "dataset.info()"
      ],
      "metadata": {
        "id": "-S3OdaUirLZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se puede notar que los **valores nulos** han desaparecido, adem√°s de haber disminuido el n√∫mero de **atributos**, cambiado sus nombres, y reducido el n√∫mero de ejemplos de entrenamiento.\n",
        "\n",
        "Ahora dirijamos nuestra atenci√≥n a la secci√≥n con el t√≠tulo \"**Dtype**\", la cual nos muestra el tipo de datos que contiene cada columna; en esta notaremos algunos valores de tipo **enteros** (int64) o **racionales** (float64), es decir, de tipo cuantitativo; sin embargo, tambi√©n notaremos algunas columnas con valores **[cualitativos](https://dialektico.com/datos-machine-learning/#Dcualit)**, como la correspondiente al **tipo de motor**.\n",
        "\n",
        "Dado que en nuestros **objetivos** se ha establecido que estos datos se utilizar√°n como **entradas** para un modelo de **aprendizaje autom√°tico**, los datos de todo el conjunto deber√°n ser de **tipo num√©rico**, por lo que procederemos a transformar todos los datos cualitativos en representaciones matem√°ticas de los mismos.\n"
      ],
      "metadata": {
        "id": "Gln6Kl9Psa5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Para llevar a cabo esta transformaci√≥n utilizaremos la librer√≠a **[Sklearn](https://scikit-learn.org/stable/index.html)** y su clase **[LabelEncoder()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)**, la cual asignar√° un n√∫mero entero a cada uno de los posibles valores de la columna.\n",
        "\n",
        "**Nota:** utilizaremos esta t√©cnica para mantener la sencillez de la experiencia del preprocesamiento, mas no significa que sea la mejor opci√≥n entre todo el espectro de posibilidades. La forma de generar representaciones num√©ricas para cada variable depender√° de factores asociados al modelo del cual ser√°n entradas. Esto lo aprenderemos con detalle m√°s adelante."
      ],
      "metadata": {
        "id": "uCCqfZ4qu1Kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observemos c√≥mo se aplica esto a la caracter√≠stica \"**Tipo_de_motor**\".\n",
        "\n",
        "Partiremos observando cu√°les son los diferentes **valores** de este atributo:"
      ],
      "metadata": {
        "id": "7nvxuF3GvuR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se imprimen los distintos valores de la columna Tipo_de_motor, y el n√∫mero de veces que aparece cada uno.\n",
        "dataset['Tipo_de_motor'].value_counts()"
      ],
      "metadata": {
        "id": "dHW69-KivAT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la primer columna se observa el valor que puede tomar el **atributo**, y en la segunda columna el **n√∫mero de veces** que aparece cada uno. Por ejemplo, hay 131 renglones o ejemplos de entrenamiento en los que aparece \"ohc\" como valor del **tipo de motor**.\n",
        "\n",
        "Sabiendo que existen 6 posibles valores, procedemos a codificarlos num√©ricamente:"
      ],
      "metadata": {
        "id": "Vre5mOakwF3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se declara el codificador.\n",
        "le = preprocessing.LabelEncoder()\n",
        "\n",
        "# Se transforman los valores de la columna en enteros.\n",
        "dataset['Tipo_de_motor'] = le.fit_transform(dataset['Tipo_de_motor'])\n",
        "\n",
        "# Se imprime el conjunto de datos.\n",
        "dataset"
      ],
      "metadata": {
        "id": "fpf-YEJFcTjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la tabla se puede constatar que ahora los valores para la variable \"Tipo_de_motor\" son **num√©ricos**: enteros que se han asignado a cada posible dato cualitativo.\n",
        "\n",
        "La transformaci√≥n se dio de la siguiente forma:\n",
        "\n",
        "\n",
        "*   **dohc** = 0\n",
        "*   **l** = 1\n",
        "*   **ohc** = 2\n",
        "*   **ohcf** = 3\n",
        "*   **ohcv** = 4\n",
        "*   **rotor** = 5\n",
        "\n"
      ],
      "metadata": {
        "id": "sXKrqjZ-w-Ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despu√©s de ver c√≥mo se da esta transformaci√≥n, procedemos a aplicar la misma a las dem√°s columnas con datos de tipo cualitativo:"
      ],
      "metadata": {
        "id": "ZNMXkCyWjPGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se aplica el codificador a cada columna con valores no num√©ricos.\n",
        "dataset['N√∫mero_de_cilindros'] = le.fit_transform(dataset['N√∫mero_de_cilindros'])\n",
        "dataset['Sistema_de_combustible'] = le.fit_transform(dataset['Sistema_de_combustible'])\n",
        "\n",
        "# Se imprime el conjunto de datos.\n",
        "dataset"
      ],
      "metadata": {
        "id": "Bu3YANiWyqpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El resultado es un conjunto de datos con todos los valores de tipo **num√©ricos**."
      ],
      "metadata": {
        "id": "gPDOC3c0zQoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "44Uw44Kaa20Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Escalado de datos y an√°lisis descriptivo**"
      ],
      "metadata": {
        "id": "reH3KFjrzX0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora que tenemos valores completamente num√©ricos, procedemos a explorarlos de otra forma: realizando un breve an√°lisis descriptivo desplegando algunas estad√≠sticas b√°sicas utilizando la funci√≥n **describe()** de Pandas, la cual nos mostrar√° la siguiente informaci√≥n de los valores de cada columna:\n",
        "\n",
        "\n",
        "\n",
        "*   **Count**: n√∫mero de valores de cada columna.\n",
        "*   **Mean**: su promedio.\n",
        "*   **Std**: su desviaci√≥n est√°ndar.\n",
        "*   **Min**: su valor m√≠nimo.\n",
        "*   **Max**: su valor m√°ximo.\n",
        "*   **25%, 50%, 75%**: sus percentiles\n"
      ],
      "metadata": {
        "id": "ParUWmvPzexZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Desplegamos las estad√≠sticas del conjunto de datos.\n",
        "dataset.describe()"
      ],
      "metadata": {
        "id": "QMnQwRPL74fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la tabla puedes observar los valores de cada estad√≠stica para cada atributo o columna existente.\n",
        "\n",
        "Por el momento, nos centraremos en los valores de los rangos (m√≠nimos y m√°ximos). Sin contar los datos de aquellas variables que transformamos en el paso anterior, ya que sus valores son poco diversos y todos **discretos**, se pueden discernir cantidades **m√°ximas** y **m√≠nimas** muy distantes entre s√≠ (en un rango de 48 a 25,552).\n",
        "\n",
        "Dato que estos datos fungir√°n como **entradas** para un **algoritmo** de machine learning, lo recomendable es **escalar** los datos (hacer m√°s peque√±as las distancias entre las observaciones coloc√°ndolos en un intervalo) para eficientar su **procesamiento**."
      ],
      "metadata": {
        "id": "GQtPtDi2zzJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para esto recurriremos a la clase **[MinMaxScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)** de Sklearn, la cual escalar√° los datos a un intervalo de 0 a 1:"
      ],
      "metadata": {
        "id": "G0XPOlqBozwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se declara la clase que escalar√° nuestros datos.\n",
        "scaler = preprocessing.MinMaxScaler()\n",
        "\n",
        "# Se escalan los datos a un intervalo entre 0 y 1.\n",
        "normalized = scaler.fit_transform(dataset)\n",
        "\n",
        "# Se toman los valores de las columnas y se recrea la tabla con los datos escalados.\n",
        "column_names = dataset.columns\n",
        "dataset = pd.DataFrame(normalized, columns = column_names)\n",
        "\n",
        "# Se imprime el conjunto de datos.\n",
        "dataset"
      ],
      "metadata": {
        "id": "DBR8lv8pnhfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podr√°s notar, el resultado es el conjunto de datos con los n√∫meros escalados entre los valores 0 y 1, lo cual minimiza las distancias y procura un mayor **rendimiento** de procesamiento."
      ],
      "metadata": {
        "id": "h-DRgGxDrQrv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "PovdQpZDbz4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Resultados**\n",
        "Hemos terminado nuestro preprocesamiento. Ahora que se ha aplicado limpieza y transformaci√≥n a nuestros datos, podemos visualizar el antes y el despu√©s de estas etapas:"
      ],
      "metadata": {
        "id": "8SdVNPTqtkLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conjunto de datos inicial:**"
      ],
      "metadata": {
        "id": "6WI9zKAhjfkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orig_dataset = pd.read_csv('https://raw.githubusercontent.com/DanielDialektico/dialektico-machine-learning-practices/refs/heads/main/data/Autom%C3%B3viles.csv')\n",
        "orig_dataset"
      ],
      "metadata": {
        "id": "jCTdADObiTzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conjunto de datos preprocesado:**"
      ],
      "metadata": {
        "id": "kQ-1q7U7jr_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "byffijjojvTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notar√°s que el conjunto de datos cambi√≥ considerablemente en cuanto a su **estructura** y **contenido**, y aunque podr√≠an hacerse m√°s adecuaciones dependiendo del objetivo fijado, este ahora est√° m√°s **optimizado** para alimentar un modelo de machine learning con los atributos solicitados."
      ],
      "metadata": {
        "id": "TrdZyLlQot5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As√≠ concluye nuestra primera **pr√°ctica** en Python, el fin de este breve encuentro fue el de mostrar qu√© tipo de cambios se pueden llegar a necesitar suscitar en los datos para un eficiente procesamiento. Realizamos tareas de an√°lisis descriptivo, inferencial, visualizaci√≥n de datos, y de limpieza y transformaci√≥n. En esta ocasi√≥n, no fueron necesarias etapas de **reducci√≥n** o **integraci√≥n**, pero no olvides que forma parte de las posibles t√©cnicas del **preprocesamiento**.\n",
        "\n",
        "No te preocupes por los detalles de todo lo que hicimos aqu√≠, ha sido meramente demostrativo. El preprocesamiento de datos es una actividad pr√°ctamente inexorable en la creaci√≥n de **modelos** de **aprendizaje de m√°quinas**, por lo que se ir√° practicando como una etapa inicial en la mayor√≠a de los algoritmos abordados en el curso.\n",
        "\n",
        "‚ñ∂ [Regresar a la lecci√≥n](https://dialektico.com/introduccion-preprocesamiento/#c%C3%B3digo) üßô"
      ],
      "metadata": {
        "id": "_5WBEo_5iR7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "bFdmIM_TcX6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ap√©ndice\n",
        "\n",
        "## Sobre el conjunto de datos\n",
        "El conjunto de datos utilizado en este ejercicio fue obtenido del Repositorio de Aprendizaje de M√°quinas UCI del Centro para Machine Learning e Inteligencia Artificial de la Universidad de California.\n",
        "\n",
        "Estos datos fueron recolectados por Jeffrey C. Schlimmer (Jeffrey.Schlimmer '@' a.gp.cs.cmu.edu), de las siguientes fuentes:\n",
        "\n",
        "*   1985 Model Import Car and Truck Specifications, 1985 Ward's Automotive Yearbook.\n",
        "*   Personal Auto Manuals, Insurance Services Office, 160 Water Street, New York, NY 10038.\n",
        "*Insurance Collision Report, Insurance Institute for Highway Safety, Watergate 600, Washington, DC 20037.\n",
        "\n",
        "**Atributos y sus rangos**:\n",
        "1. **symboling**: -3, -2, -1, 0, 1, 2, 3.\n",
        "2. **normalized-losses**: continuo de 65 a 256.\n",
        "3. **make**:\n",
        "alfa-romero, audi, bmw, chevrolet, dodge, honda,\n",
        "isuzu, jaguar, mazda, mercedes-benz, mercury,\n",
        "mitsubishi, nissan, peugot, plymouth, porsche,\n",
        "renault, saab, subaru, toyota, volkswagen, volvo\n",
        "\n",
        "4. **fuel-type**: diesel, gasolina.\n",
        "5. **aspiration**: std, turbo.\n",
        "6. **num-of-doors**: cuatro, dos.\n",
        "7. **body-style**:: techo duro, wagon, sed√°n, hatchback, descapotable.\n",
        "8. **drive-wheels**: 4x4, fwd, rwd.\n",
        "9. **engine-location**: delantero, trasero.\n",
        "10. **wheel-base**: continua de 86.6 a 120.9.\n",
        "11. **lenght**: continua de 141.1 a 208.1.\n",
        "12. **width**: continua de 60.3 a 72.3.\n",
        "13. **height**: continua de 47.8 a 59.8.\n",
        "14. **curb-weigh**: continuo de 1488 a 4066.\n",
        "15. **engine-type**: dohc, dohcv, l, ohc, ohcf, ohcv, rotor.\n",
        "16. **num-of-cylinders**: ocho, cinco, cuatro, seis, tres, doce, dos.\n",
        "17. **engine-size**: continuo de 61 a 326.\n",
        "18. **fuel-system**: 1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi.\n",
        "19. **bore**: continuo de 2.54 a 3.94.\n",
        "20. **stroke**: continua de 2.07 a 4.17.\n",
        "21. **compression-ratio**: continua de 7 a 23.\n",
        "22. **horsepower**: continua de 48 a 288.\n",
        "23. **peak-rpm**: continua de 4.150 a 6.600 rpm.\n",
        "24. **city-mpg**: continuo de 13 a 49.\n",
        "25. **highway-mpg**: continuo de 16 a 54."
      ],
      "metadata": {
        "id": "HaVTloDvHZTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MIT License\n",
        "#\n",
        "# Copyright (c) 2023 Daniel Garc√≠a\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ],
      "metadata": {
        "id": "APuJgH91GaF7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}